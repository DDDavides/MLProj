{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#import needed Python libraries\n",
    "import numpy as np\n",
    "import csv\n",
    "import random\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as scistats\n",
    "import math\n",
    "import pylab\n",
    "import statsmodels as sm\n",
    "\n",
    "#graphics parameters of the notebook\n",
    "# display graphs inline\n",
    "%matplotlib inline\n",
    "\n",
    "# Make graphs prettier\n",
    "pd.set_option('display.max_columns', 15)\n",
    "pd.set_option('display.width', 400)\n",
    "pd.set_option('plotting.matplotlib.register_converters', True)\n",
    "\n",
    "# Make the fonts bigger\n",
    "plt.rc('figure', figsize=(14, 7))\n",
    "plt.rc('font', family='normal', weight='bold', size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#inegrate data from 2009-2010 to 2018-2019 seasons from different files\n",
    "data_18_19 = pd.read_csv(\"./data/2018_2019.csv\", parse_dates=True)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
    "data_17_18 = pd.read_csv(\"./data/2017_2018.csv\", parse_dates=True)\n",
    "data_16_17 = pd.read_csv(\"./data/2016_2017.csv\", parse_dates=True)\n",
    "data_15_16 = pd.read_csv(\"./data/2015_2016.csv\", parse_dates=True)\n",
    "data_14_15 = pd.read_csv(\"./data/2014_2015.csv\", parse_dates=True)\n",
    "data_13_14 = pd.read_csv(\"./data/2013_2014.csv\", parse_dates=True)\n",
    "data_12_13 = pd.read_csv(\"./data/2012_2013.csv\", parse_dates=True)\n",
    "data_11_12 = pd.read_csv(\"./data/2011_2012.csv\", parse_dates=True)\n",
    "data_10_11 = pd.read_csv(\"./data/2010_2011.csv\", parse_dates=True)\n",
    "data_09_10 = pd.read_csv(\"./data/2009_2010.csv\", parse_dates=True)\n",
    "data_08_09 = pd.read_csv(\"./data/2008_2009.csv\", parse_dates=True)\n",
    "#data_07_08 = pd.read_csv(\"./data/2007_2008.csv\", parse_dates=True)\n",
    "#data_06_07 = pd.read_csv(\"./data/2006_2007.csv\", parse_dates=True)\n",
    "#data_05_06 = pd.read_csv(\"./data/2005_2006.csv\", parse_dates=True)\n",
    "\n",
    "\n",
    "\n",
    "#test about data consistency for all files\n",
    "for df in [data_18_19, data_17_18, data_16_17, data_15_16, data_14_15, data_13_14, data_12_13, data_11_12, data_10_11, data_09_10, data_08_09]:\n",
    "    print(\"Number of df columns : \" + str(len(df.columns)))\n",
    "\n",
    "#integrate data in a single df\n",
    "raw_data = pd.concat([data_18_19, data_17_18, data_16_17, data_15_16, data_14_15, data_13_14, data_12_13, data_11_12, data_10_11, data_09_10, data_08_09])\n",
    "\n",
    "print(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Select useful features for datavisualization and analysis purposes\n",
    "E0_data = raw_data[[\"Date\", \"HomeTeam\", \"AwayTeam\", \"FTHG\", \"FTAG\",\n",
    "                    \"FTR\", \"HTAG\", 'B365A', 'B365D', 'B365H', 'BSA', \n",
    "                    'BSD', 'BSH', 'BWA', 'BWD', 'BWH', 'GBA', 'GBD',\n",
    "                    'GBH', 'IWA', 'IWD', 'IWH', 'LBA', 'LBD', 'LBH',\n",
    "                    'PSA', 'PSD', 'PSH', 'SBA', 'SBD', 'SBH', 'SJA',\n",
    "                    'SJD', 'SJH', 'VCA', 'VCD', 'VCH', 'WHA','WHD', 'WHH']]\n",
    "\n",
    "#convert date format to YYYY-MM-DD classic format\n",
    "E0_data.Date = E0_data.Date.map(lambda x : \"20\" + x[6:8] + \"-\" + x[3:5] + \"-\" + x[0:2])\n",
    "\n",
    "#sort data by date\n",
    "E0_data.sort_values('Date', inplace=True)\n",
    "\n",
    "#reset data indexes\n",
    "E0_data = E0_data.reset_index(drop=True)\n",
    "\n",
    "#create matchID column\n",
    "E0_data['matchID'] = E0_data.index\n",
    "\n",
    "#create season feature\n",
    "E0_data['Season'] = 0\n",
    "E0_data.Season = E0_data.Date.map(lambda x : int(x[0:4]) if int(x[5:7]) > 6 else int(x[0:4]) - 1)\n",
    "\n",
    "#null values test\n",
    "E0_data.isnull().any()\n",
    "\n",
    "#create teams list\n",
    "teams = E0_data['HomeTeam'].unique()\n",
    "print(teams)\n",
    "\n",
    "#create seasons list\n",
    "seasons = np.sort(E0_data['Season'].unique())\n",
    "print(seasons)\n",
    "\n",
    "#create dictionary containing teams list by season\n",
    "teams_by_season = {season : E0_data[E0_data['Season'] == season]['HomeTeam'].unique() for season in seasons}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#match day feature construction for HomeTeam and AwayTeam (1st match of a season --> 1, last --> 38 because 20 team play by season)\n",
    "E0_HT_grpby = E0_data.groupby('HomeTeam')[['Date']]\n",
    "E0_AT_grpby = E0_data.groupby('AwayTeam')[['Date']]\n",
    "\n",
    "# Calcola la giornata di campionato\n",
    "def fxyH(row):\n",
    "    x = row['HomeTeam']\n",
    "    y = row['Date']\n",
    "    df1 = E0_HT_grpby.get_group(x)\n",
    "    df2 = E0_AT_grpby.get_group(x)\n",
    "    df1 = df1[df1['Date'] < y]\n",
    "    df2 = df2[df2['Date'] < y]\n",
    "    day = (1 + len(df1) + len(df2)) % 38\n",
    "    return 38 if day == 0 else day \n",
    "\n",
    "def fxyA(row):\n",
    "    x = row['AwayTeam']\n",
    "    y = row['Date']\n",
    "    df1 = E0_HT_grpby.get_group(x)\n",
    "    df2 = E0_AT_grpby.get_group(x)\n",
    "    df1 = df1[df1['Date'] < y]\n",
    "    df2 = df2[df2['Date'] < y]\n",
    "    day = (1 + len(df1) + len(df2)) % 38\n",
    "    return 38 if day == 0 else day \n",
    "\n",
    "E0_data['HomeTeamDay'] = E0_data.apply(fxyH, axis=1)\n",
    "E0_data['AwayTeamDay'] = E0_data.apply(fxyA, axis=1)\n",
    "\n",
    "# Calcolo il numero di partite giocate in casa e fuori per ogni team in ogni stagione\n",
    "E0_data['ones'] = 1\n",
    "for season in seasons:\n",
    "    for team in teams_by_season[season]:\n",
    "        sH = E0_data[(E0_data['HomeTeam'] == team) & (E0_data['Season'] == season)]['ones']\n",
    "        E0_data.loc[sH.index, 'HomeTeamHomeDay'] = sH.cumsum()\n",
    "        \n",
    "        sA = E0_data[(E0_data['AwayTeam'] == team) & (E0_data['Season'] == season)]['ones']\n",
    "        E0_data.loc[sA.index, 'AwayTeamAwayDay'] = sA.cumsum()\n",
    "        \n",
    "# Nel dataset ho una colonna FTR (full time result) che riporta H se ha vinto la squadra in casa, altrimenti A\n",
    "def resultConverter(A):\n",
    "    if A == 'H':\n",
    "        return 'W'\n",
    "    elif A =='A':\n",
    "        return 'L'\n",
    "    else:\n",
    "        return 'D'\n",
    "\n",
    "def resultInverser(A):\n",
    "    if A == 'W':\n",
    "        return 'L'\n",
    "    elif A == 'L':\n",
    "        return 'W'\n",
    "    else:\n",
    "        return 'D'\n",
    "def ordinalResultConverter(A):\n",
    "    if A == 'W':\n",
    "        return 1\n",
    "    elif A == 'L':\n",
    "        return 0\n",
    "    else:\n",
    "        return 0.5\n",
    "    \n",
    "#make dummies variables for FTR (result of match), HW = Home Win, AW = Away Win, D = draw\n",
    "E0_data['HW'] = E0_data.FTR.map(lambda x : 1 if x == 'H' else 0)\n",
    "E0_data['AW'] = E0_data.FTR.map(lambda x : 1 if x == 'A' else 0)\n",
    "E0_data['D']= E0_data.FTR.map(lambda x : 1 if x == 'D' else 0)\n",
    "\n",
    "#make 2 different variable for the result of a match : 1 for the home team point of view, the other for the away team pt of view\n",
    "E0_data['HR'] = E0_data.FTR.map(lambda x : resultConverter(x))\n",
    "E0_data['AR'] = E0_data.HR.map(lambda x : resultInverser(x))\n",
    "\n",
    "#make ordinal variable for the home team point of view result (1 = win, 0.5 = Draw, 0 = loss)\n",
    "E0_data['ordinalHR'] = E0_data.HR.map(lambda x : ordinalResultConverter(x))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "grp_by_HT = E0_data.groupby('HomeTeam')\n",
    "grp_by_AT = E0_data.groupby('AwayTeam')\n",
    "\n",
    "grp_by_HT_and_season = E0_data.groupby(['HomeTeam', 'Season'])\n",
    "grp_by_AT_and_season = E0_data.groupby(['AwayTeam', 'Season'])\n",
    "\n",
    "#past performance features engineering\n",
    "for team in teams:\n",
    "    \n",
    "    #we retrieve results series of the team\n",
    "    teamHomeResults_s = grp_by_HT.get_group(team)['HR']\n",
    "    teamAwayResults_s = grp_by_AT.get_group(team)['AR']\n",
    "    #combine these 2 series and sort the obtained serie\n",
    "    teamResults_s = pd.concat([teamHomeResults_s, teamAwayResults_s]).sort_index()\n",
    "\n",
    "    #(i) compute k_last_HR and k_last_AR --> 6 features\n",
    "    # Dizionario {<partita>:<risultato>}\n",
    "    # {0: nan, 21: 'W', 49: 'W', 69: 'L', 96: 'W', 113: 'D', ..., 4073: 'W', 4097: 'W', 4113: 'W', 4142: 'W', 4166: 'L'}\n",
    "    # Shifto a dx di 1, 2 e 3 per avere accesso, rispettivamente, alle ultima partita, le ultime 2 e le ultime 3 ??????\n",
    "    lag1TeamResults_d = teamResults_s.shift(1).to_dict()\n",
    "    lag2TeamResults_d = teamResults_s.shift(2).to_dict()\n",
    "    lag3TeamResults_d = teamResults_s.shift(3).to_dict()\n",
    "    \n",
    "    #k_last_HTR and k_last_ATR are just shifted versions of the results series\n",
    "    E0_data.loc[teamHomeResults_s.index,'1_last_HTR'] = E0_data.loc[teamHomeResults_s.index,:].index.map(lambda x : lag1TeamResults_d[x])\n",
    "    E0_data.loc[teamHomeResults_s.index,'2_last_HTR'] = E0_data.loc[teamHomeResults_s.index,:].index.map(lambda x : lag2TeamResults_d[x])\n",
    "    E0_data.loc[teamHomeResults_s.index,'3_last_HTR'] = E0_data.loc[teamHomeResults_s.index,:].index.map(lambda x : lag3TeamResults_d[x])\n",
    "    E0_data.loc[teamAwayResults_s.index,'1_last_ATR'] = E0_data.loc[teamAwayResults_s.index,:].index.map(lambda x : lag1TeamResults_d[x])\n",
    "    E0_data.loc[teamAwayResults_s.index,'2_last_ATR'] = E0_data.loc[teamAwayResults_s.index,:].index.map(lambda x : lag2TeamResults_d[x])\n",
    "    E0_data.loc[teamAwayResults_s.index,'3_last_ATR'] = E0_data.loc[teamAwayResults_s.index,:].index.map(lambda x : lag3TeamResults_d[x])\n",
    "    \n",
    "    #(ii) Compute k_last_HTRH and k_last ATAR --> 4 features\n",
    "    #we need here to diferentiate home results and past results. Python dictionaries allows the program to access to\n",
    "    #needed data faster than with a Pandas serie\n",
    "    # Dovendo predire il risultato finale, assumiamo che sia più dipendente dai risultati a fine match (full time - FT), \n",
    "    # piuttosto che dai risultati a metà match (half time - HT) => consideriamo quindi sono gli half time dell'ultima e penultima partita\n",
    "    lag1TeamHomeResults_d = teamHomeResults_s.shift(1).to_dict()\n",
    "    lag2TeamHomeResults_d = teamHomeResults_s.shift(2).to_dict()\n",
    "    lag1TeamAwayResults_d = teamAwayResults_s.shift(1).to_dict()\n",
    "    lag2TeamAwayResults_d = teamAwayResults_s.shift(2).to_dict()\n",
    "    \n",
    "    E0_data.loc[teamHomeResults_s.index,'1_last_HTHR'] = E0_data.loc[teamHomeResults_s.index,:].index.map(lambda x : lag1TeamHomeResults_d[x])\n",
    "    E0_data.loc[teamHomeResults_s.index,'2_last_HTHR'] = E0_data.loc[teamHomeResults_s.index,:].index.map(lambda x : lag2TeamHomeResults_d[x])\n",
    "    E0_data.loc[teamAwayResults_s.index,'1_last_ATAR'] = E0_data.loc[teamAwayResults_s.index,:].index.map(lambda x : lag1TeamAwayResults_d[x])\n",
    "    E0_data.loc[teamAwayResults_s.index,'2_last_ATAR'] = E0_data.loc[teamAwayResults_s.index,:].index.map(lambda x : lag2TeamAwayResults_d[x])\n",
    "    \n",
    "    #(iii) rates based features : we need to get only season specific results series (to avoid taking previous season results into season rates)\n",
    "    for season in seasons:\n",
    "        \n",
    "        if team in teams_by_season[season]:\n",
    "            #retrieve season specific results serie (1 win serie, 1 draw serie the loss  will be computed thanks to\n",
    "            #the 2 others)\n",
    "            teamHomeResultsW_s = grp_by_HT_and_season.get_group((team,season))['HW']\n",
    "            teamAwayResultsW_s = grp_by_AT_and_season.get_group((team,season))['AW']\n",
    "            teamResultsW_s = pd.concat([teamHomeResultsW_s, teamAwayResultsW_s]).sort_index()\n",
    "\n",
    "            teamHomeResultsD_s = grp_by_HT_and_season.get_group((team,season))['D']\n",
    "            teamAwayResultsD_s = grp_by_AT_and_season.get_group((team,season))['D']\n",
    "            teamResultsD_s = pd.concat([teamHomeResultsD_s, teamAwayResultsD_s]).sort_index()\n",
    "        \n",
    "            #(0) compute HW rates, HL rates, AW rates, LW rates since begining of season\n",
    "            teamResultsWCumul_d = teamResultsW_s.shift(1).cumsum().to_dict()\n",
    "            teamResultsDCumul_d = teamResultsD_s.shift(1).cumsum().to_dict()\n",
    "\n",
    "            #(i) compute 7_HTW_rate, 12_HTW_rate, 7_HTD_rate, 12_HTD_rate, 7_ATW_rate, 12_ATW_rate, 7_ATD_rate, 12_ATD_rate --> 8 features\n",
    "            # Esempio del 7_HTW_rate:\n",
    "            #   1. Mi posiziono su una partita X e voglio calcolare la media M dei risultati precedenti alla partita (X-1, X-2, ..., X-7), quindi shifto a dx\n",
    "            #       per non considerare nella M la partita stessa X e per avere M nella stessa posizione di X (sfrutto le stesse matrici su cui eseguo i calcoli) \n",
    "            #   2. Poi si fissa una finestra larga 7, e si calcola la media di un sottoinsieme di dati, ma per farlo è necessario \n",
    "            #       ci siano almeno 5 valori nella finestra. La finestra parte con il sottoinsieme di dati {0}, poi {0, 15} -> {0, 15, 21} -> ...\n",
    "            # {<matchID>: <media>}\n",
    "            # {0: nan, 15: nan, 21: nan, 30: nan, 43: nan, 49: 0.8, 60: 0.6666666666666666, 69: 0.5714285714285714, ...}\n",
    "            win7TeamResultsW_d = teamResultsW_s.shift(1).rolling(window = 7, min_periods = 5).mean().to_dict()\n",
    "            win12TeamResultsW_d = teamResultsW_s.shift(1).rolling(window = 12, min_periods = 8).mean().to_dict()\n",
    "            win7TeamResultsD_d = teamResultsD_s.shift(1).rolling( window = 7, min_periods = 5).mean().to_dict()\n",
    "            win12TeamResultsD_d = teamResultsD_s.shift(1).rolling( window = 12, min_periods = 8).mean().to_dict()\n",
    "        \n",
    "            E0_data.loc[teamHomeResultsW_s.index,'HTW_rate'] = E0_data.loc[teamHomeResultsW_s.index,:].index.map(lambda x : teamResultsWCumul_d[x])\n",
    "            E0_data.loc[teamAwayResultsW_s.index,'ATW_rate'] = E0_data.loc[teamAwayResultsW_s.index,:].index.map(lambda x : teamResultsWCumul_d[x])\n",
    "            E0_data.loc[teamHomeResultsW_s.index,'HTD_rate'] = E0_data.loc[teamHomeResultsW_s.index,:].index.map(lambda x : teamResultsDCumul_d[x])\n",
    "            E0_data.loc[teamAwayResultsW_s.index,'ATD_rate'] = E0_data.loc[teamAwayResultsW_s.index,:].index.map(lambda x : teamResultsDCumul_d[x])\n",
    "        \n",
    "            E0_data.loc[teamHomeResultsW_s.index,'7_HTW_rate'] = E0_data.loc[teamHomeResultsW_s.index,:].index.map(lambda x : win7TeamResultsW_d[x])\n",
    "            E0_data.loc[teamHomeResultsW_s.index,'12_HTW_rate'] = E0_data.loc[teamHomeResultsW_s.index,:].index.map(lambda x : win12TeamResultsW_d[x])\n",
    "            E0_data.loc[teamAwayResultsW_s.index,'7_ATW_rate'] = E0_data.loc[teamAwayResultsW_s.index,:].index.map(lambda x : win7TeamResultsW_d[x])\n",
    "            E0_data.loc[teamAwayResultsW_s.index,'12_ATW_rate'] = E0_data.loc[teamAwayResultsW_s.index,:].index.map(lambda x : win12TeamResultsW_d[x])\n",
    "        \n",
    "            E0_data.loc[teamHomeResultsD_s.index,'7_HTD_rate'] = E0_data.loc[teamHomeResultsD_s.index,:].index.map(lambda x : win7TeamResultsD_d[x])\n",
    "            E0_data.loc[teamHomeResultsD_s.index,'12_HTD_rate'] = E0_data.loc[teamHomeResultsD_s.index,:].index.map(lambda x : win12TeamResultsD_d[x])\n",
    "            E0_data.loc[teamAwayResultsD_s.index,'7_ATD_rate'] = E0_data.loc[teamAwayResultsD_s.index,:].index.map(lambda x : win7TeamResultsD_d[x])\n",
    "            E0_data.loc[teamAwayResultsD_s.index,'12_ATD_rate'] = E0_data.loc[teamAwayResultsD_s.index,:].index.map(lambda x : win12TeamResultsD_d[x])\n",
    "\n",
    "        #(ii) compute 5_HTHW_rate and 5_ATAW_rate\n",
    "        win5TeamResultsHomeW_d = teamHomeResultsW_s.shift(1).rolling( window = 5, min_periods = 3).mean().to_dict()\n",
    "        win5TeamResultsAwayW_d = teamAwayResultsW_s.shift(1).rolling( window = 5, min_periods = 3).mean().to_dict()\n",
    "        E0_data.loc[teamHomeResultsW_s.index,'5_HTHW_rate'] = E0_data.loc[teamHomeResultsW_s.index,:].index.map(lambda x : win5TeamResultsHomeW_d[x])\n",
    "        E0_data.loc[teamAwayResultsW_s.index,'5_ATAW_rate'] = E0_data.loc[teamAwayResultsW_s.index,:].index.map(lambda x : win5TeamResultsAwayW_d[x])\n",
    "        \n",
    "        #(iii) compute HTHW_rate, ATAW_rate, HTHD_rate, ATAD_rate\n",
    "        teamHomeResultsCumulW_d = teamHomeResultsW_s.shift(1).cumsum().to_dict()\n",
    "        teamHomeResultsCumulD_d = teamHomeResultsD_s.shift(1).cumsum().to_dict()\n",
    "        teamAwayResultsCumulW_d = teamAwayResultsW_s.shift(1).cumsum().to_dict()\n",
    "        teamAwayResultsCumulD_d = teamAwayResultsD_s.shift(1).cumsum().to_dict()\n",
    "        E0_data.loc[teamHomeResultsW_s.index,'HTHW_rate'] = E0_data.loc[teamHomeResultsW_s.index,:].index.map(lambda x : teamHomeResultsCumulW_d[x])\n",
    "        E0_data.loc[teamHomeResultsW_s.index,'HTHD_rate'] = E0_data.loc[teamHomeResultsW_s.index,:].index.map(lambda x : teamHomeResultsCumulD_d[x])\n",
    "        E0_data.loc[teamAwayResultsW_s.index,'ATAW_rate'] = E0_data.loc[teamAwayResultsW_s.index,:].index.map(lambda x : teamAwayResultsCumulW_d[x])\n",
    "        E0_data.loc[teamAwayResultsW_s.index,'ATAD_rate'] = E0_data.loc[teamAwayResultsW_s.index,:].index.map(lambda x : teamAwayResultsCumulD_d[x])\n",
    "\n",
    "\n",
    "        \n",
    "#compute missing features k_XTL_rate thanks to the k_XTW_rate and k_XTD_rate features\n",
    "E0_data.loc[:,'7_HTL_rate'] = 1 - (E0_data['7_HTW_rate'] + E0_data['7_HTD_rate'])\n",
    "E0_data.loc[:,'12_HTL_rate'] = 1 - (E0_data['7_HTW_rate'] + E0_data['7_HTD_rate'])\n",
    "E0_data.loc[:,'7_ATL_rate'] = 1 - (E0_data['7_ATW_rate'] + E0_data['7_ATD_rate'])\n",
    "E0_data.loc[:,'12_ATL_rate'] = 1 - (E0_data['7_ATW_rate'] + E0_data['7_ATD_rate'])\n",
    "\n",
    "#compute missing HTL_rate, ATL_rate with features with the wins and draws features\n",
    "E0_data.loc[:,'HTW_rate'] = E0_data['HTW_rate']/E0_data['HomeTeamDay']\n",
    "E0_data.loc[:,'ATW_rate'] = E0_data['ATW_rate']/E0_data['AwayTeamDay']\n",
    "E0_data.loc[:,'HTD_rate'] = E0_data['HTD_rate']/E0_data['HomeTeamDay']\n",
    "E0_data.loc[:,'ATD_rate'] = E0_data['ATD_rate']/E0_data['AwayTeamDay']\n",
    "E0_data.loc[:,'HTL_rate'] = 1 - (E0_data['HTW_rate'] + E0_data['HTD_rate'])\n",
    "E0_data.loc[:,'ATL_rate'] = 1 - (E0_data['ATW_rate'] + E0_data['ATD_rate'])\n",
    "\n",
    "#we finish to compute HTHW_rate, ..., ATAD_rate features and compute corresponding loss features\n",
    "E0_data.loc[:,'HTHW_rate'] = E0_data['HTHW_rate']/E0_data['HomeTeamHomeDay']\n",
    "E0_data.loc[:,'ATAW_rate'] = E0_data['ATAW_rate']/E0_data['AwayTeamAwayDay']\n",
    "E0_data.loc[:,'HTHD_rate'] = E0_data['HTHD_rate']/E0_data['HomeTeamHomeDay']\n",
    "E0_data.loc[:,'ATAD_rate'] = E0_data['ATAD_rate']/E0_data['AwayTeamAwayDay']\n",
    "E0_data.loc[:,'HTHL_rate'] = 1 - (E0_data['HTHW_rate'] + E0_data['HTHD_rate'])\n",
    "E0_data.loc[:,'ATAL_rate'] = 1 - (E0_data['ATAW_rate'] + E0_data['ATAD_rate'])\n",
    "\n",
    "E0_data.to_csv(\"./data/features.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elo = valore di ranking per definire l'andamento di una squadra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Elo ranking method parameters\n",
    "k = 20.0\n",
    "d = 400.0\n",
    "c = 10.0\n",
    "\n",
    "#Initialization of output containers\n",
    "ELO_dict = dict()\n",
    "gammaHT_dict = dict()\n",
    "gammaAT_dict = dict()\n",
    "\n",
    "#intermediate data containers initilization\n",
    "latest_update_d = dict() #contains latest updates in date of ELO_dict\n",
    "prev_ELO_score_d = dict() #contains latest ELO_score given to a team for computing new one\n",
    "\n",
    "prev_season_teams = [team for team in teams] #contains list of teams for the current season\n",
    "last_teams_ELO_av = 0.0 #contains ELO average of last previous season teams\n",
    "\n",
    "for team in teams:\n",
    "    latest_update_d[team] = '2001-01-01'\n",
    "    prev_ELO_score_d[team] = 0.0\n",
    "\n",
    "for season in E0_data.Season.unique():\n",
    "    season_match_dates = E0_data[E0_data['Season'] == season].Date.unique()\n",
    "    season_teams = E0_data[E0_data['Season'] == season].HomeTeam.unique()\n",
    "    last_season_date = season_match_dates[len(season_match_dates) - 1]\n",
    "    \n",
    "    for Steam in season_teams:\n",
    "        if not (Steam in prev_season_teams):\n",
    "            prev_ELO_score_d[Steam] = last_teams_ELO_av\n",
    "            \n",
    "    for date in season_match_dates:\n",
    "        for team in teams:\n",
    "            # Se non trovo, per una certa data un certo team, allora per quel team e quella data l'ELO non cambia\n",
    "            if not ((team in E0_data[E0_data['Date'] == date]['HomeTeam'].values) | (team in E0_data[E0_data['Date'] == date]['AwayTeam'].values)):\n",
    "                ELO_dict[(team, date)] = prev_ELO_score_d[team]\n",
    "                latest_update_d[team] = date\n",
    "            # Altrimenti lo aggiorno sulla base del risultato\n",
    "            else:\n",
    "                if latest_update_d[team] < date:\n",
    "                    Hteam = E0_data[(E0_data['Date'] == date) & ((E0_data['HomeTeam'] == team) | (E0_data['AwayTeam'] == team))]['HomeTeam'].values[0]\n",
    "                    Ateam = E0_data[(E0_data['Date'] == date) & ((E0_data['HomeTeam'] == team) | (E0_data['AwayTeam'] == team))]['AwayTeam'].values[0]\n",
    "            \n",
    "                    l0H = prev_ELO_score_d[Hteam]\n",
    "                    l0A = prev_ELO_score_d[Ateam]\n",
    "                    gammaH = 1.0/(1.0 + c**((l0A - l0H)/d))\n",
    "                    gammaA = 1.0 - gammaH\n",
    "                    # .values ritorna un array quindi devo prendere il valore e non l'indice\n",
    "                    alphaH = E0_data[(E0_data['Date'] == date) & (E0_data['HomeTeam'] == Hteam)]['ordinalHR'].values[0]\n",
    "                    alphaA = 1 - alphaH\n",
    "                    \n",
    "                    #compute new scores\n",
    "                    new_HT_ELO_score = l0H + k * (alphaH - gammaH)\n",
    "                    new_AT_ELO_score = l0A + k * (alphaA - gammaA)\n",
    "\n",
    "                    #put new scores in ELO_dict\n",
    "                    ELO_dict[(Hteam, date)] = new_HT_ELO_score\n",
    "                    ELO_dict[(Ateam, date)] = new_AT_ELO_score\n",
    "                    gammaHT_dict[(Hteam, date)] = gammaH\n",
    "                    gammaAT_dict[(Ateam, date)] = gammaA\n",
    "                    latest_update_d[Hteam] = date\n",
    "                    latest_update_d[Ateam] = date\n",
    "            \n",
    "                    #update prev_ELO_score_d and latest_update_d\n",
    "                    prev_ELO_score_d[Hteam] = new_HT_ELO_score\n",
    "                    prev_ELO_score_d[Ateam] = new_AT_ELO_score\n",
    "        \n",
    "        if date == last_season_date:\n",
    "            ELOs = np.array([prev_ELO_score_d[Steam] for Steam in season_teams])\n",
    "            ELOs.sort()\n",
    "            last_teams_ELO_av = np.mean(ELOs[0:-17])\n",
    "            prev_season_teams = season_teams\n",
    "            \n",
    "                        \n",
    "#make HTeamEloScore, ATeamEloScore and gammaHome features from previously computed dictionaries\n",
    "\n",
    "def HomeTeamEloScore(row):\n",
    "    return ELO_dict[(row['HomeTeam'], row['Date'])]\n",
    "\n",
    "def AwayTeamEloScore(row):\n",
    "    return ELO_dict[(row['AwayTeam'], row['Date'])]\n",
    "\n",
    "def gammaHTeamDate(row):\n",
    "    return gammaHT_dict[(row['HomeTeam'], row['Date'])]\n",
    "\n",
    "#compute resulting Elo scores important features\n",
    "E0_data.loc[:,'HTeamEloScore'] = E0_data.apply(HomeTeamEloScore, axis=1) \n",
    "E0_data.loc[:,'ATeamEloScore'] = E0_data.apply(AwayTeamEloScore, axis=1) \n",
    "E0_data.loc[:,'gammaHome'] = E0_data.apply(gammaHTeamDate, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%% Team tiredness feature extraction\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for team in teams:\n",
    "    homeMatchDates_s = E0_data[E0_data['HomeTeam'] == team]['Date']\n",
    "    awayMatchDates_s = E0_data[E0_data['AwayTeam'] == team]['Date']\n",
    "    matchDates_s = pd.concat([homeMatchDates_s, awayMatchDates_s]).sort_index()\n",
    "    lastMatchDates_s = matchDates_s.shift(1)\n",
    "    matchDates = matchDates_s.values\n",
    "        \n",
    "    E0_data.loc[E0_data['HomeTeam'] == team, 'HTLastMatchDate'] = E0_data.loc[E0_data['HomeTeam'] == team].index.map(lambda x : lastMatchDates_s[x])\n",
    "    E0_data.loc[E0_data['AwayTeam'] == team, 'ATLastMatchDate'] = E0_data.loc[E0_data['AwayTeam'] == team].index.map(lambda x : lastMatchDates_s[x])\n",
    "    \n",
    "def HTdaysBetweenDates(row):\n",
    "    if not (pd.isnull(row['HTLastMatchDate'])):\n",
    "        currDate = pd.to_datetime(row['Date'])\n",
    "        prevDate = pd.to_datetime(row['HTLastMatchDate'])\n",
    "        ndays = (currDate - prevDate).days \n",
    "        if ndays < 20:\n",
    "            return ndays\n",
    "        else: \n",
    "            return np.nan\n",
    "    else:\n",
    "        return np.nan \n",
    "    \n",
    "def ATdaysBetweenDates(row):\n",
    "    if not (pd.isnull(row['ATLastMatchDate'])):\n",
    "        currDate = pd.to_datetime(row['Date'])\n",
    "        prevDate = pd.to_datetime(row['ATLastMatchDate'])\n",
    "        return (currDate - prevDate).days\n",
    "    else:\n",
    "        return np.nan \n",
    "    \n",
    "E0_data.loc[:, 'HTdaysSinceLastMatch'] = E0_data.apply(HTdaysBetweenDates, axis=1)\n",
    "E0_data.loc[:, 'ATdaysSinceLastMatch'] = E0_data.apply(ATdaysBetweenDates, axis=1)\n",
    "E0_data.loc[:,'DaysSinceLastMatchRate'] = E0_data['HTdaysSinceLastMatch'].astype(float)/E0_data['ATdaysSinceLastMatch'].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "E0_data['1_last_HTR_isW'] = E0_data['1_last_HTR'].map(lambda x : 1 if x == 'W' else 0)\n",
    "E0_data['1_last_HTR_isL'] = E0_data['1_last_HTR'].map(lambda x : 1 if x == 'L' else 0) \n",
    "E0_data['2_last_HTR_isW'] = E0_data['2_last_HTR'].map(lambda x : 1 if x == 'W' else 0)\n",
    "E0_data['2_last_HTR_isL'] = E0_data['2_last_HTR'].map(lambda x : 1 if x == 'L' else 0) \n",
    "E0_data['3_last_HTR_isW'] = E0_data['3_last_HTR'].map(lambda x : 1 if x == 'W' else 0)\n",
    "E0_data['3_last_HTR_isL'] = E0_data['3_last_HTR'].map(lambda x : 1 if x == 'L' else 0) \n",
    "\n",
    "E0_data['1_last_ATR_isW'] = E0_data['1_last_ATR'].map(lambda x : 1 if x == 'W' else 0)\n",
    "E0_data['1_last_ATR_isL'] = E0_data['1_last_ATR'].map(lambda x : 1 if x == 'L' else 0) \n",
    "E0_data['2_last_ATR_isW'] = E0_data['2_last_ATR'].map(lambda x : 1 if x == 'W' else 0)\n",
    "E0_data['2_last_ATR_isL'] = E0_data['2_last_ATR'].map(lambda x : 1 if x == 'L' else 0) \n",
    "E0_data['3_last_ATR_isW'] = E0_data['3_last_ATR'].map(lambda x : 1 if x == 'W' else 0)\n",
    "E0_data['3_last_ATR_isL'] = E0_data['3_last_ATR'].map(lambda x : 1 if x == 'L' else 0) \n",
    "\n",
    "E0_data['1_last_HTHR_isW'] = E0_data['1_last_HTHR'].map(lambda x : 1 if x == 'W' else 0)\n",
    "E0_data['1_last_HTHR_isL'] = E0_data['1_last_HTHR'].map(lambda x : 1 if x == 'L' else 0) \n",
    "E0_data['2_last_HTHR_isW'] = E0_data['2_last_HTHR'].map(lambda x : 1 if x == 'W' else 0)\n",
    "E0_data['2_last_HTHR_isL'] = E0_data['2_last_HTHR'].map(lambda x : 1 if x == 'L' else 0)\n",
    "\n",
    "E0_data['1_last_ATAR_isW'] = E0_data['1_last_ATAR'].map(lambda x : 1 if x == 'W' else 0)\n",
    "E0_data['1_last_ATAR_isL'] = E0_data['1_last_ATAR'].map(lambda x : 1 if x == 'L' else 0) \n",
    "E0_data['2_last_ATAR_isW'] = E0_data['2_last_ATAR'].map(lambda x : 1 if x == 'W' else 0)\n",
    "E0_data['2_last_ATAR_isL'] = E0_data['2_last_ATAR'].map(lambda x : 1 if x == 'L' else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Home wins, Away wins and draws rates variations over seasons\n",
    "HW_rates = []\n",
    "AW_rates = []\n",
    "D_rates = []\n",
    "\n",
    "for season in seasons:\n",
    "    season_data = E0_data[E0_data['Season'] == season]\n",
    "    total_matches_nb = len(season_data.index)\n",
    "    HW_rate = float(len(season_data[season_data['FTR'] == 'H'].index))/float(total_matches_nb)\n",
    "    AW_rate = float(len(season_data[season_data['FTR'] == 'A'].index))/float(total_matches_nb)\n",
    "    D_rate = float(len(season_data[season_data['FTR'] == 'D'].index))/float(total_matches_nb)\n",
    "    HW_rates.append(HW_rate)\n",
    "    AW_rates.append(AW_rate)\n",
    "    D_rates.append(D_rate)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(seasons, HW_rates, label=\"Home wins rate\")\n",
    "plt.plot(seasons, AW_rates, label=\"Away wins rate\")\n",
    "plt.plot(seasons, D_rates, label=\"Draw rates\")\n",
    "plt.legend()\n",
    "plt.xticks([int(season) for season in seasons], seasons)\n",
    "plt.xlabel(\"Seasons\")\n",
    "plt.ylabel(\"Home wins, draws and losses rates\")\n",
    "plt.title(\"Home wins, draws and losses rates evolution over seasons\")\n",
    "\n",
    "#global wins, draws and losses rates\n",
    "HW_rate = float(len(E0_data[E0_data['HR'] == 'W'].index))/float(len(E0_data.index))\n",
    "HL_rate = float(len(E0_data[E0_data['HR'] == 'L'].index))/float(len(E0_data.index))\n",
    "HD_rate = float(len(E0_data[E0_data['HR'] == 'D'].index))/float(len(E0_data.index))\n",
    "rates = [HW_rate, HL_rate, HD_rate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "width = 0.12\n",
    "xticks = np.array([0.3, 1.0, 1.7])\n",
    "xticksLabels = ['0.0 - 0.2', '0.2 - 0.4', '0.4 - 0.6']\n",
    "xlabels = [[\"Last home team result = W\", \"Last home team result = D\", \"Last home team result = L\"],\n",
    "           [\"2nd last home team result = W\", \"2nd last home team result = D\", \"2nd last home team result = L\"],\n",
    "           [\"Last away team result = W\", \"Last away team result = D\", \"Last away team result = L\"],\n",
    "            [\"2nd last away team result = W\", \"2nd last away team result = D\", \"2nd last away team result = L\"]]\n",
    "\n",
    "var_names = ['1_last_HTR', '2_last_HTR', '1_last_ATR', '2_last_ATR']\n",
    "titles = [\"Home results repartition vs. lagged performance values (\" + var_name + \")\" for var_name in var_names]\n",
    "\n",
    "k=0\n",
    "for var_name in var_names:\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    HW_rates = []\n",
    "    HD_rates = []\n",
    "    \n",
    "    for Rvalue in ['W','D','L']:\n",
    "        HW_rates.append(len(E0_data[(E0_data[var_name] == Rvalue) & (E0_data['HR'] == 'W')].index) / len(E0_data[E0_data[var_name] == Rvalue].index))\n",
    "        HD_rates.append(len(E0_data[(E0_data[var_name] == Rvalue) & (E0_data['HR'] == 'D')].index) / len(E0_data[E0_data[var_name] == Rvalue].index))\n",
    "    \n",
    "    HL_rates = [1 - (HW_rate + HD_rate) for (HW_rate, HD_rate) in zip(HW_rates, HD_rates)]\n",
    "    \n",
    "    plt.bar(xticks - 1.5 * width, HW_rates, width, color = '#6495ED')\n",
    "    plt.bar(xticks - 0.5 * width, HD_rates, width, color = '#778899')\n",
    "    plt.bar(xticks + 0.5 * width, HL_rates, width, color = '#FF7F50')\n",
    "    plt.plot((0.025, 2.0), (HW_rate, HW_rate), '--', color='#6495ED', linewidth=2)\n",
    "    plt.plot((0.025, 2.0), (HD_rate, HD_rate), '--', color='#778899', linewidth=2)\n",
    "    plt.plot((0.025, 2.0), (HL_rate, HL_rate), '--', color='#FF7F50', linewidth=2)\n",
    "    \n",
    "    pylab.xticks(xticks, xlabels[k])\n",
    "    plt.ylabel(\"Wins, draws and victories rates\")\n",
    "    plt.legend([\"HT Wins rate\", \"HT Draws rate\", \"HT Losses rate\"])\n",
    "    plt.xlim(0.0, 2.0)\n",
    "    plt.title(titles[k], fontsize=17)\n",
    "\n",
    "    k += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%    \n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "bars = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "width = (0.2 - 0.05)/3.0\n",
    "xticks = np.array([0.025, 0.225, 0.425, 0.625, 0.825, 1.025])\n",
    "xticksLabels = ['0.0 - 0.2', '0.2 - 0.4', '0.4 - 0.6', '0.6 - 0.8', '0.8 - 1.0', '1.0']\n",
    "xlabels = [\"Home Team average Wins rate for last 7 days\", \"Home Team average Wins rate for last 12 days\", \n",
    "          \"Away team average Wins rate for last 7 days\", \"Away team average Wins rate for last 12 days\"]    \n",
    "var_names = ['7_HTW_rate', '12_HTW_rate', '7_ATW_rate', '12_ATW_rate']\n",
    "titles = [\"Home results repartition vs. home/away wins rate over past results (\" + var_name + \")\" for var_name in var_names]\n",
    "\n",
    "\n",
    "k=0\n",
    "for var_name in var_names:\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    nonzero_bars = []\n",
    "    nonzero_xticks = []\n",
    "    HW_rates = []\n",
    "    HD_rates = []\n",
    "    \n",
    "    for (bar, xtick) in zip(bars, xticks):\n",
    "        if len(E0_data[(E0_data[var_name] >= bar) & (E0_data[var_name] < (bar + 0.2))].index) > 0:\n",
    "            nonzero_xticks.append(xtick)\n",
    "            HW_rates.append(len(E0_data[(E0_data[var_name] >= bar) & (E0_data[var_name] < (bar + 0.2)) & (E0_data['HR'] == 'W')].index) / len(E0_data[(E0_data[var_name] >= bar) & (E0_data[var_name] < (bar + 0.2))].index))\n",
    "            HD_rates.append(len(E0_data[(E0_data[var_name] >= bar) & (E0_data[var_name] < (bar + 0.2)) & (E0_data['HR'] == 'D')].index) / len(E0_data[(E0_data[var_name] >= bar) & (E0_data[var_name] < (bar + 0.2))].index))\n",
    "    \n",
    "    HL_rates = [1 - (HW_rate + HD_rate) for (HW_rate, HD_rate) in zip(HW_rates, HD_rates)]\n",
    "    \n",
    "    plt.bar(np.array(nonzero_xticks), HW_rates, width, color = '#6495ED')\n",
    "    plt.bar(np.array(nonzero_xticks) + width, HD_rates, width, color = '#778899')\n",
    "    plt.bar(np.array(nonzero_xticks) + 2*width, HL_rates, width, color = '#FF7F50')\n",
    "    plt.plot((0.025, 1.2), (HW_rate, HW_rate), '--', color='#6495ED', linewidth=2)\n",
    "    plt.plot((0.025, 1.2), (HD_rate, HD_rate), '--', color='#778899', linewidth=2)\n",
    "    plt.plot((0.025, 1.2), (HL_rate, HL_rate), '--', color='#FF7F50', linewidth=2)\n",
    "\n",
    "    pylab.xticks([0.1, 0.3, 0.5, 0.7, 0.9, 1.1], xticksLabels)\n",
    "    plt.xlabel(xlabels[k])\n",
    "    plt.ylabel(\"Wins, draws and losses rates\")\n",
    "    plt.legend([\"HT Wins rate\", \"HT Draws rate\", \"HT Losses rate\"])\n",
    "    plt.title(titles[k], fontsize=17)\n",
    "\n",
    "\n",
    "\n",
    "    k += 1    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%    \n"
    }
   },
   "outputs": [],
   "source": [
    "# Elo rank feature visualization\n",
    "\n",
    "plt.hist(E0_data['gammaHome'].values, bins=20)\n",
    "plt.xlabel(\"gammaHome values\")\n",
    "plt.ylabel(\"Bins sizes\")\n",
    "plt.xlim(0,1)\n",
    "plt.title(\"Elo score distribution histogram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "#random teams elo score visualization\n",
    "random_teams = [teams[random.randint(0, len(teams))-1] for k in range(0,5)]\n",
    "for team in random_teams:\n",
    "    HomeEloScores = E0_data[E0_data['HomeTeam'] == team]['HTeamEloScore']\n",
    "    AwayEloScores = E0_data[E0_data['AwayTeam'] == team]['ATeamEloScore']\n",
    "    EloScores = pd.concat([HomeEloScores, AwayEloScores]).sort_index()\n",
    "    EloScores.plot()\n",
    "plt.legend(random_teams)\n",
    "plt.xlabel('Match number')\n",
    "plt.ylabel('Elo scores')\n",
    "plt.title('Visualization of Elo scores evolution over seasons for 5 random teams', fontsize = 16)\n",
    "    \n",
    "fig = plt.figure()\n",
    "for team in ['Arsenal', 'Man United', 'Liverpool']:\n",
    "    HomeEloScores = E0_data[E0_data['HomeTeam'] == team]['HTeamEloScore']\n",
    "    AwayEloScores = E0_data[E0_data['AwayTeam'] == team]['ATeamEloScore']\n",
    "    EloScores = pd.concat([HomeEloScores, AwayEloScores]).sort_index()\n",
    "    EloScores.plot()\n",
    "plt.legend(['Arsenal', 'Man United', 'Liverpool'])\n",
    "plt.xlabel('Match number')\n",
    "plt.ylabel('Elo scores')\n",
    "plt.title('Visualization of Elo scores evolution over seasons for Arsenal, Man United and Liverpool', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#we define bins of approximatively equal sizes in terms of Elo score\n",
    "bars = [0.0, 0.266, 0.35, 0.43, 0.48, 0.52, 0.62, 0.71, 1.0]\n",
    "bars_pos = [0.0 + k * (1.0/8.0) for k in range(1,9)]\n",
    "width = 0.02\n",
    "xticks = np.array([(bars[k+1] - bars[k])/2.0 for k in range(0, len(bars) - 1)])\n",
    "xticksLabels = ['0.0 - 0.266', '0.266 - 0.35','0.35 - 0.43', '0.43-0.48', '0.48 - 0.52','0.52-0.62', '0.62 - 0.71', '0.71 - 1.0']\n",
    "xlabels = [\"Home Team average home-wins rate for last 5 days (at home)\", \"Away Team average away-wins rate for last 5 days (away)\"]    \n",
    "\n",
    "HW_rates = []\n",
    "HD_rates = []\n",
    "HL_rates = []\n",
    "\n",
    "for k in range(0 , len(bars) - 1):\n",
    "    HW_rates.append(len(E0_data[(E0_data['gammaHome'] >= bars[k]) & (E0_data['gammaHome'] < bars[k+1]) & (E0_data['HR'] == 'W')].index) / len(E0_data[(E0_data['gammaHome'] >= bars[k]) & (E0_data['gammaHome'] < bars[k+1])].index))\n",
    "    HD_rates.append(len(E0_data[(E0_data['gammaHome'] >= bars[k]) & (E0_data['gammaHome'] < bars[k+1]) & (E0_data['HR'] == 'D')].index) / len(E0_data[(E0_data['gammaHome'] >= bars[k]) & (E0_data['gammaHome'] < bars[k+1])].index))\n",
    "    HL_rates = [1 - (HW_rate + HD_rate) for (HW_rate, HD_rate) in zip(HW_rates, HD_rates)]\n",
    "    \n",
    "plt.bar(np.array(bars_pos), HW_rates, width, color = '#6495ED', alpha = 0.85)\n",
    "plt.bar(np.array(bars_pos) + width, HD_rates, width, color = '#778899', alpha = 0.85)\n",
    "plt.bar(np.array(bars_pos) + 2*width, HL_rates, width, color = '#FF7F50', alpha = 0.85)\n",
    "\n",
    "plt.plot((0.0, 1.2), (HW_rate, HW_rate), '--', color='#6495ED', linewidth=2)\n",
    "plt.plot((0.0, 1.2), (HD_rate, HD_rate), '--', color='#778899', linewidth=2)\n",
    "plt.plot((0.0, 1.2), (HL_rate, HL_rate), '--', color='#FF7F50', linewidth=2)\n",
    "    \n",
    "plt.ylabel(\"Wins, draws and losses rates\")\n",
    "plt.legend([\"HT Wins rate\", \"HT Draws rate\", \"HT Losses rate\"])\n",
    "plt.xlim(0,1.2)\n",
    "pylab.xticks(np.array(bars_pos) + width, xticksLabels, rotation=45)\n",
    "plt.title(\"Effect of gammaHome score match home results repartition\", fontsize=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "#we define bins of minimum size for HTdaysSinceLastMatch\n",
    "bins = [0, 3, 4, 5, 6, 7, 8, 9, 11, 14, 15, 21]\n",
    "bars_pos = [0.0 + k * (20/12.0) for k in range(1,12)]\n",
    "width = 0.25\n",
    "xticksLabels = ['0 - 2', '3', '4', '5', '6', '7','8', '9-10', '11-13', '14', '15-20']\n",
    "\n",
    "HW_rates = []\n",
    "HD_rates = []\n",
    "HL_rates = []\n",
    "\n",
    "for k in range(0 , len(bins) - 1):\n",
    "    HW_rates.append(len(E0_data[(E0_data['HTdaysSinceLastMatch'] >= bins[k]) & (E0_data['HTdaysSinceLastMatch'] < bins[k+1]) & (E0_data['HR'] == 'W')].index) / len(E0_data[(E0_data['HTdaysSinceLastMatch'] >= bins[k]) & (E0_data['HTdaysSinceLastMatch'] < bins[k+1])].index))\n",
    "    HD_rates.append(len(E0_data[(E0_data['HTdaysSinceLastMatch'] >= bins[k]) & (E0_data['HTdaysSinceLastMatch'] < bins[k+1]) & (E0_data['HR'] == 'D')].index) / len(E0_data[(E0_data['HTdaysSinceLastMatch'] >= bins[k]) & (E0_data['HTdaysSinceLastMatch'] < bins[k+1])].index))\n",
    "    HL_rates = [1 - (HW_rate + HD_rate) for (HW_rate, HD_rate) in zip(HW_rates, HD_rates)]\n",
    "    \n",
    "plt.bar(np.array(bars_pos), HW_rates, width, color = '#6495ED')\n",
    "plt.bar(np.array(bars_pos) + width, HD_rates, width, color = '#778899')\n",
    "plt.bar(np.array(bars_pos) + 2*width, HL_rates, width, color = '#FF7F50')\n",
    "\n",
    "plt.plot((0.0, 21), (HW_rate, HW_rate), '--', color='#6495ED', linewidth=2)\n",
    "plt.plot((0.0, 21), (HD_rate, HD_rate), '--', color='#778899', linewidth=2)\n",
    "plt.plot((0.0, 21), (HL_rate, HL_rate), '--', color='#FF7F50', linewidth=2)\n",
    "\n",
    "pylab.xticks(np.array(bars_pos) + width, xticksLabels, rotation=45)\n",
    "plt.ylabel(\"Wins, draws and losses rates\")\n",
    "plt.legend([\"HT Wins rate\", \"HT Draws rate\", \"HT Losses rate\"])\n",
    "plt.xlim(0,21)\n",
    "plt.title(\"Influence of HTdaysSinceLastMatch over results repartition\", fontsize = 17)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "E0_data_copy = E0_data[['HW', 'AW', 'D',\n",
    "                        'gammaHome',\n",
    "                        'DaysSinceLastMatchRate',\n",
    "                        'HTW_rate', 'HTD_rate', 'HTL_rate',\n",
    "                        'ATW_rate', 'ATD_rate', 'ATL_rate',\n",
    "                        'HTHW_rate', 'HTHD_rate', 'HTHL_rate',\n",
    "                        'ATAW_rate', 'ATAD_rate', 'ATAL_rate',\n",
    "                        '1_last_HTR', '2_last_HTR',\n",
    "                        '1_last_ATR', '2_last_ATR',\n",
    "                        '1_last_HTHR', '2_last_HTHR',\n",
    "                        '1_last_ATAR', '2_last_ATAR',\n",
    "                        '7_HTW_rate', '12_HTW_rate', \n",
    "                        '7_ATW_rate', '12_ATW_rate', \n",
    "                        '7_HTL_rate', '12_HTL_rate', \n",
    "                        '7_ATL_rate', '12_ATD_rate',\n",
    "                        '7_HTD_rate', '12_HTD_rate',\n",
    "                        '7_ATD_rate', '7_ATD_rate',\n",
    "                       '5_HTHW_rate', '5_ATAW_rate']]\n",
    "\n",
    "#make dummies variables for the past results indicators features (3 for W, D and L)\n",
    "E0_data_copy.loc[:, '1_last_HTRisW'] = E0_data_copy['1_last_HTR'].map(lambda x : 1 if x == 'W' else 0)\n",
    "E0_data_copy.loc[:, '1_last_HTRisD'] = E0_data_copy['1_last_HTR'].map(lambda x : 1 if x == 'D' else 0)\n",
    "E0_data_copy.loc[:, '1_last_HTRisL'] = E0_data_copy['1_last_HTR'].map(lambda x : 1 if x == 'L' else 0)\n",
    "E0_data_copy.loc[:, '2_last_HTRisW'] = E0_data_copy['2_last_HTR'].map(lambda x : 1 if x == 'W' else 0)\n",
    "E0_data_copy.loc[:, '2_last_HTRisD'] = E0_data_copy['2_last_HTR'].map(lambda x : 1 if x == 'D' else 0)\n",
    "E0_data_copy.loc[:, '2_last_HTRisL'] = E0_data_copy['2_last_HTR'].map(lambda x : 1 if x == 'L' else 0)\n",
    "E0_data_copy.loc[:, '1_last_ATRisW'] = E0_data_copy['1_last_ATR'].map(lambda x : 1 if x == 'W' else 0)\n",
    "E0_data_copy.loc[:, '1_last_ATRisD'] = E0_data_copy['1_last_ATR'].map(lambda x : 1 if x == 'D' else 0)\n",
    "E0_data_copy.loc[:, '1_last_ATRisL'] = E0_data_copy['1_last_ATR'].map(lambda x : 1 if x == 'L' else 0)\n",
    "E0_data_copy.loc[:, '2_last_ATRisW'] = E0_data_copy['2_last_ATR'].map(lambda x : 1 if x == 'W' else 0)\n",
    "E0_data_copy.loc[:, '2_last_ATRisD'] = E0_data_copy['2_last_ATR'].map(lambda x : 1 if x == 'D' else 0)\n",
    "E0_data_copy.loc[:, '2_last_ATRisL'] = E0_data_copy['2_last_ATR'].map(lambda x : 1 if x == 'L' else 0)\n",
    "\n",
    "E0_data_copy.loc[:, '1_last_HTHRisW'] = E0_data_copy['1_last_HTHR'].map(lambda x : 1 if x == 'W' else 0)\n",
    "E0_data_copy.loc[:, '1_last_HTHRisD'] = E0_data_copy['1_last_HTHR'].map(lambda x : 1 if x == 'D' else 0)\n",
    "E0_data_copy.loc[:, '1_last_HTHRisL'] = E0_data_copy['1_last_HTHR'].map(lambda x : 1 if x == 'L' else 0)\n",
    "E0_data_copy.loc[:, '2_last_HTHRisW'] = E0_data_copy['2_last_HTHR'].map(lambda x : 1 if x == 'W' else 0)\n",
    "E0_data_copy.loc[:, '2_last_HTHRisD'] = E0_data_copy['2_last_HTHR'].map(lambda x : 1 if x == 'D' else 0)\n",
    "E0_data_copy.loc[:, '2_last_HTHRisL'] = E0_data_copy['2_last_HTHR'].map(lambda x : 1 if x == 'L' else 0)\n",
    "E0_data_copy.loc[:, '1_last_ATARisW'] = E0_data_copy['1_last_ATAR'].map(lambda x : 1 if x == 'W' else 0)\n",
    "E0_data_copy.loc[:, '1_last_ATARisD'] = E0_data_copy['1_last_ATAR'].map(lambda x : 1 if x == 'D' else 0)\n",
    "E0_data_copy.loc[:, '1_last_ATARisL'] = E0_data_copy['1_last_ATAR'].map(lambda x : 1 if x == 'L' else 0)\n",
    "E0_data_copy.loc[:, '2_last_ATARisW'] = E0_data_copy['2_last_ATAR'].map(lambda x : 1 if x == 'W' else 0)\n",
    "E0_data_copy.loc[:, '2_last_ATARisD'] = E0_data_copy['2_last_ATAR'].map(lambda x : 1 if x == 'D' else 0)\n",
    "E0_data_copy.loc[:, '2_last_ATARisL'] = E0_data_copy['2_last_ATAR'].map(lambda x : 1 if x == 'L' else 0)\n",
    "\n",
    "#under matrix form\n",
    "corr = E0_data_copy.corr()\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#and in a more understandable and graphical form\n",
    "def plot_corr(df,size=10):\n",
    "    corr = df.corr()\n",
    "    fig, ax = plt.subplots(figsize=(size, size))\n",
    "    ax.matshow(corr)\n",
    "    plt.xticks(range(len(corr.columns)), corr.columns, rotation = 90);\n",
    "    plt.yticks(range(len(corr.columns)), corr.columns);\n",
    "    \n",
    "plot_corr(E0_data_copy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "eloSeasons = [2005, 2006, 2007, 2008]\n",
    "trainingSeasons = [2009, 2010, 2011, 2012, 2013, 2014, 2015]\n",
    "testSeasons = [2016, 2017,2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "featuresPool = ['HR', '1_last_HTR_isW', '1_last_HTR_isL', '2_last_HTR_isW', '2_last_HTR_isL',\n",
    "                '3_last_HTR_isW', '3_last_HTR_isL',\n",
    "                '1_last_ATR_isW', '1_last_ATR_isL', '2_last_ATR_isW', '2_last_ATR_isL', \n",
    "                '3_last_ATR_isW',  '3_last_ATR_isL', \n",
    "                '1_last_HTHR_isW', '1_last_HTHR_isL', '2_last_HTHR_isW', '2_last_HTHR_isL',\n",
    "                '1_last_ATAR_isW', '1_last_ATAR_isL', '2_last_ATAR_isW', '2_last_ATAR_isL',\n",
    "                '7_HTW_rate', '12_HTW_rate', '7_ATW_rate', '12_ATW_rate', \n",
    "                '7_HTD_rate', '12_HTD_rate', '7_ATD_rate', '12_ATD_rate',\n",
    "                '7_HTL_rate', '12_HTL_rate', '7_ATL_rate', '12_ATL_rate',\n",
    "                '5_HTHW_rate', '5_ATAW_rate',  'gammaHome']\n",
    "\n",
    "eloSeasons = [2005, 2006, 2007, 2008]\n",
    "trainingSeasons = [2009, 2010, 2011, 2012, 2013, 2014, 2015]\n",
    "testSeasons = [2016, 2017,2018]\n",
    "\n",
    "E0_data_tr = E0_data.loc[(E0_data['Season'].isin(trainingSeasons)) & (E0_data['HomeTeamDay'] > 4) & (E0_data['AwayTeamDay'] > 4), featuresPool]\n",
    "E0_data_te = E0_data.loc[(E0_data['Season'].isin(testSeasons)) & (E0_data['HomeTeamDay'] > 4) & (E0_data['AwayTeamDay'] > 4), featuresPool]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Folds generations custom function \n",
    "\n",
    "\n",
    "def foldsGenerator(ixSet, foldMinSize, foldMaxSize, trInitSize, trOptimalSize = -1):\n",
    "    \n",
    "    subsetsList = []\n",
    "    subsetsList.append(ixSet[0:trInitSize])\n",
    "    Nsubsets = 1\n",
    "    \n",
    "    ixSetLength = ixSet.size\n",
    "    \n",
    "    unfoldedSetSize = ixSetLength - trInitSize\n",
    "    prevSubsetStop = trInitSize\n",
    "    \n",
    "    while (unfoldedSetSize > foldMaxSize):\n",
    "        nextFoldSize = random.randint(foldMinSize, foldMaxSize)\n",
    "        \n",
    "        subsetsList.append(ixSet[prevSubsetStop:(prevSubsetStop + nextFoldSize)])\n",
    "        \n",
    "        unfoldedSetSize -= nextFoldSize\n",
    "        prevSubsetStop += nextFoldSize\n",
    "        Nsubsets += 1\n",
    "    \n",
    "    subsetsList.append(ixSet[prevSubsetStop:])\n",
    "    Nsubsets += 1    \n",
    "    return (Nsubsets, subsetsList)\n",
    "\n",
    "#test\n",
    "#sub = foldsGenerator(E0_data_tr.index, 40, 55, 700)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Scores functions implementation\n",
    "\n",
    "def brierScore(probW, probL, probD, true, classLabels):\n",
    "    \n",
    "    trueW = true.map(lambda x : 1 if x == classLabels['W'] else 0).values\n",
    "    trueL = true.map(lambda x : 1 if x == classLabels['L'] else 0).values\n",
    "    trueD = true.map(lambda x : 1 if x == classLabels['D'] else 0).values\n",
    "    \n",
    "    cumulScore = (probW - trueW)*(probW - trueW) + (probL - trueL)*(probL - trueL) + (probD - trueD)*(probD - trueD)\n",
    "    \n",
    "    return float(np.sum(cumulScore))/float(true.index.size)\n",
    "\n",
    "def rankProbabilityScore(probW, probL, probD, true, classLabels):\n",
    "    trueW = true.map(lambda x : 1 if x == classLabels['W'] else 0).values\n",
    "    trueL = true.map(lambda x : 1 if x == classLabels['L'] else 0).values\n",
    "    trueD = true.map(lambda x : 1 if x == classLabels['D'] else 0).values\n",
    "    \n",
    "    true1 = trueL\n",
    "    true2 = trueL + trueD\n",
    "    \n",
    "    prob1 = probL\n",
    "    prob2 = probL + probD\n",
    "    \n",
    "    cumulScore = (prob1 - true1)*(prob1 - true1) + (prob2 - true2) * (prob2 - true2)\n",
    "    \n",
    "    return(float(np.sum(cumulScore))/(2.0 * float(true.index.size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Score estimation function \n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def crossValidation(predictors, labels, classLabels, foldGenParams, fitPredFun, fittingParams, scoreFunList, NObsMax = -1):\n",
    "    \n",
    "    (Nfold, subsetsList) = foldsGenerator(labels.index, foldGenParams['foldMinSize'], \n",
    "                                          foldGenParams['foldMaxSize'], foldGenParams['trInitSize'])\n",
    "    trSubset = []\n",
    "    validSubset = subsetsList.pop(0)\n",
    "    \n",
    "    k=0\n",
    "    avScores = np.zeros(3 + len(scoreFunList))\n",
    "    while (len(subsetsList) > 0) :\n",
    "        if (k == 0):                \n",
    "            trSubset = validSubset\n",
    "        else:\n",
    "            trSubset = trSubset.append(validSubset)\n",
    "        if (NObsMax != (-1)):\n",
    "            trSubset = trSubset[-NObsMax:]\n",
    "        \n",
    "        validSubset = subsetsList.pop(0)\n",
    "    \n",
    "        predictors_tr = predictors.loc[trSubset,:]\n",
    "        predictors_val = predictors.loc[validSubset,:]\n",
    "        labels_tr = labels.loc[trSubset]\n",
    "        labels_val = labels.loc[validSubset]\n",
    "\n",
    "        #model fitting + probabilities prediction\n",
    "        (predLabels, probW, probL, probD) = fitPredFun(predictors_tr, labels_tr, predictors_val, labels_val, fittingParams)\n",
    "\n",
    "        scores = []\n",
    "        scores.append(float(len(predictors_val.index)) * accuracy_score(predLabels, labels_val.values))\n",
    "        \n",
    "        if len(labels_val.unique()) > 2:\n",
    "            scores.append(float(len(predictors_val.index)) * precision_score(predLabels, labels_val, average='weighted'))\n",
    "            scores.append(float(len(predictors_val.index)) * recall_score(predLabels, labels_val, average='weighted'))\n",
    "        else:\n",
    "            scores.append(float(len(predictors_val.index)) * precision_score(predLabels, labels_val, pos_label='W'))\n",
    "            scores.append(float(len(predictors_val.index)) * recall_score(predLabels, labels_val, pos_label='W'))\n",
    "        \n",
    "        for scoreFun in scoreFunList:\n",
    "            scores.append(float(len(predictors_val.index)) * scoreFun(probW, probL, probD, labels_val, classLabels))\n",
    "        print(scoreFun(probW, probL, probD, labels_val, classLabels))\n",
    "        avScores = avScores + np.array(scores)\n",
    "        \n",
    "        k += 1\n",
    "    return avScores/float(len(labels.index) - foldGenParams['trInitSize'])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from IPython.display import display\n",
    "\n",
    "#Create X and Y\n",
    "X = pd.get_dummies(E0_data[['HomeTeam', 'AwayTeam', 'HTeamEloScore', 'ATeamEloScore', 'HTdaysSinceLastMatch',\n",
    "                            'ATdaysSinceLastMatch', 'HTW_rate', 'ATW_rate',\n",
    "                            'ATD_rate', 'HTD_rate', \n",
    "                '7_HTW_rate', '12_HTW_rate', '7_ATW_rate', '12_ATW_rate', \n",
    "                '7_HTD_rate', '12_HTD_rate', '7_ATD_rate', '12_ATD_rate',\n",
    "                '7_HTL_rate', '12_HTL_rate', '7_ATL_rate', '12_ATL_rate',\n",
    "                '5_HTHW_rate', '5_ATAW_rate']])\n",
    "Y = E0_data[['FTR']]\n",
    "\n",
    "#X preprocessing\n",
    "imputer = SimpleImputer()\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "#Split X and Y into training and Test Sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_imputed, Y, shuffle=True)\n",
    "\n",
    "#Logistic Regression Model Setup\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "#Logistic Regression Model Metrics\n",
    "print(\"Logestic Regression\")\n",
    "print(\"Train Score: \", model.score(x_train, y_train))\n",
    "print(\"Test Score: \", model.score(x_test, y_test))\n",
    "print(classification_report(y_test, model.predict(x_test), digits=3))\n",
    "\n",
    "#Forest model setup\n",
    "forest = RandomForestClassifier(n_estimators=2, random_state=2)\n",
    "forest.fit(x_train, y_train)\n",
    "\n",
    "#Forest Model Metrics\n",
    "print(\"Forest Classifier\")\n",
    "print(\"Train Score: \", forest.score(x_train, y_train))\n",
    "print(\"Test Score: \", forest.score(x_test, y_test))\n",
    "print(classification_report(y_test, forest.predict(x_test), digits=3))\n",
    "\n",
    "print(y_test.shape)\n",
    "print(forest.predict(x_test).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "display(E0_data.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "#make TF work like the TF in the HWs\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "#Other feature selections\n",
    "'''#X = pd.get_dummies(E0_data[['HomeTeam', 'AwayTeam', 'HTeamEloScore', 'ATeamEloScore', 'HTdaysSinceLastMatch',\n",
    "                            'ATdaysSinceLastMatch']])\n",
    "X = pd.get_dummies(E0_data[['HomeTeam', 'AwayTeam', 'FTAG', 'FTHG']])\n",
    "X = pd.get_dummies(E0_data[['HomeTeam', 'AwayTeam', 'FTR']])\n",
    "X = pd.get_dummies(E0_data.drop(['FTR',  'HTAG', 'Date', 'matchID', 'HW', 'AW', 'D', 'AR',\n",
    "                                 'ordinalHR', 'Season'], axis=1))'''\n",
    "\n",
    "#Setup Data\n",
    "X = pd.get_dummies(E0_data[['HomeTeam', 'AwayTeam', 'HTeamEloScore', 'ATeamEloScore', 'HTdaysSinceLastMatch',\n",
    "                            'ATdaysSinceLastMatch', 'HTW_rate', 'ATW_rate',\n",
    "                            'ATD_rate', 'HTD_rate', \n",
    "                '7_HTW_rate', '12_HTW_rate', '7_ATW_rate', '12_ATW_rate', \n",
    "                '7_HTD_rate', '12_HTD_rate', '7_ATD_rate', '12_ATD_rate',\n",
    "                '7_HTL_rate', '12_HTL_rate', '7_ATL_rate', '12_ATL_rate',\n",
    "                '5_HTHW_rate', '5_ATAW_rate']])\n",
    "\n",
    "Y = E0_data[['ordinalHR']].to_numpy().ravel()*2\n",
    "\n",
    "\n",
    "\n",
    "#X preprocessing\n",
    "imputer = SimpleImputer()\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "#Split X and Y into training and Test Sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_imputed, Y, shuffle=True)\n",
    "\n",
    "#Neural Network Setup\n",
    "n_inputs = X.shape[1]\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 3\n",
    "\n",
    "#Tensorflow X and Y\n",
    "X = tf.placeholder(tf.float32, shape = (None, n_inputs), name = 'X')\n",
    "Y = tf.placeholder(tf.int32, shape = (None), name = 'Y')\n",
    "\n",
    "#Function used to better display model metrics\n",
    "def convert_ordinalHR(x):\n",
    "    y=[]\n",
    "    for i in range(x.size):\n",
    "        if x[i] == 0:\n",
    "            y.append('A')\n",
    "        elif x[i] == 2:\n",
    "            y.append('H')\n",
    "        elif x[i] == 1:\n",
    "            y.append('D')\n",
    "    return y\n",
    "\n",
    "#General Function for neural layer setup\n",
    "def neuron_layer(X, n_neurons, name, activation = None):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        stddev = 2 / np.sqrt(n_inputs + n_neurons)\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)\n",
    "        W = tf.Variable(init, name = 'kernel')\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name='bias')\n",
    "        L2 = tf.nn.l2_loss(W)\n",
    "        Z = tf.matmul(X,W)+b\n",
    "        if activation is not None:\n",
    "            return activation(Z), L2\n",
    "        else:\n",
    "            return Z, L2\n",
    "\n",
    "#Tensorflow neural layer setup\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1, L2_1 = neuron_layer(X, n_hidden1, name='hidden1', activation=tf.nn.relu)\n",
    "    hidden2, L2_2 = neuron_layer(hidden1, n_hidden2, name='hidden2', activation=tf.nn.relu)\n",
    "    logits, L2_3 = neuron_layer(hidden2, n_outputs, name='outputs', activation=None)\n",
    "\n",
    "\n",
    "\n",
    "#Loss function\n",
    "beta=0.01\n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = Y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy+beta*(L2_1+L2_2+L2_3), name='loss')\n",
    "    \n",
    "\n",
    "\n",
    "#Optimizer\n",
    "learning_rate = 0.000001\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "#Metric analysis setup\n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits, Y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    preds = tf.argmax(input=logits, axis=1)\n",
    "    \n",
    "#Initialize the above tensorflow variables    \n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#Runtime and batchsetup\n",
    "n_epochs = 30000\n",
    "batch_size = 50\n",
    "\n",
    "#Run the model\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    #Model Loop\n",
    "    for epoch in range(n_epochs):\n",
    "        sess.run(training_op, feed_dict={X: x_train, Y: y_train})\n",
    "        acc_train = accuracy.eval(feed_dict={X: x_train, Y: y_train})\n",
    "        acc_val = accuracy.eval(feed_dict={X: x_test, Y: y_test})\n",
    "        if(epoch % 200 == 0 or epoch==n_epochs-1):\n",
    "            print(epoch, 'Train accuracy:', acc_train, 'Val accuracy:', acc_val)\n",
    "    \n",
    "    #Retrive Metrics\n",
    "    acc_val = accuracy.eval(feed_dict={X: x_test, Y: y_test})\n",
    "    preds = preds.eval(feed_dict = {X:x_test})\n",
    "    log = logits.eval(feed_dict = {X:x_test})\n",
    "    preds=convert_ordinalHR(preds)\n",
    "    y_test=convert_ordinalHR(y_test)\n",
    "    \n",
    "    #Print Metrics\n",
    "    print('Train accuracy:', acc_train, 'Val accuracy:', acc_val)\n",
    "    print(classification_report(y_test, preds, digits=3))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "#Functions to manipulate data for use in the model\n",
    "def one_hot_y(Y):\n",
    "    Y_new = np.zeros((Y.shape[0],3))\n",
    "    for i in range(Y.shape[0]-1):\n",
    "        if (Y[i] == 'H'):\n",
    "            Y_new[i]=[1,0,0]\n",
    "        elif (Y[i] == 'A'):\n",
    "            Y_new[i]=[0,1,0]\n",
    "        elif (Y[i] == 'D'):\n",
    "            Y_new[i]=[0,0,1]\n",
    "    return Y_new\n",
    "\n",
    "def revert_yoh(Y):\n",
    "    Y_new = np.empty([Y.shape[0],Y.shape[1]], dtype=\"<U1\")\n",
    "    #Y_new = np.zeros((Y.shape[0],Y.shape[1]))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            if (Y[i, j] == 0):\n",
    "                Y_new[i, j]= 'H'\n",
    "            elif (Y[i, j] == 1):\n",
    "                Y_new[i, j]= 'A'\n",
    "            elif (Y[i, j] == 2):\n",
    "                Y_new[i, j]='D'\n",
    "    return Y_new\n",
    "\n",
    "trn_ssn = [2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015]\n",
    "tst_ssn = [2016,2017,2018]\n",
    "    \n",
    "# Funzione che fa reshape\n",
    "# e.g. (a_prev=x_train, season=trn_ssn)\n",
    "def time_step(a_prev,season):\n",
    "    a_prev = a_prev[np.newaxis, ...]\n",
    "    print(a_prev.shape) # (1, 3040, ...)\n",
    "    totalMatches = len(season)*38\n",
    "    prev_f = a_prev.shape[2]\n",
    "    print(\"a_prev.shape[1]\", a_prev.shape[1]) # rows\n",
    "    print(\"a_prev.shape[2]\", a_prev.shape[2]) # columns\n",
    "    input_step = int(a_prev.shape[1]/totalMatches)\n",
    "    step = 0\n",
    "    a_new = np.zeros((totalMatches, input_step, prev_f))\n",
    "    for i in range(totalMatches):\n",
    "        # rows divise in porzioni di totalMatches rows\n",
    "        step += input_step\n",
    "        # per tutte le righe nell'intervallo di righe che stiamo guardando ora \n",
    "        # va in ogni porzione di righe di volta in volta\n",
    "        for j in range(step-input_step,step):\n",
    "            # per ogni colonna\n",
    "            for k in range(prev_f):\n",
    "                a_new[i,j-input_step*i,k] = a_prev[:,j,k]\n",
    "    return a_new\n",
    "\n",
    "    \n",
    "'''X = pd.get_dummies(E0_data[['HomeTeam', 'AwayTeam', 'HTeamEloScore', 'ATeamEloScore', 'HTdaysSinceLastMatch',\n",
    "                            'ATdaysSinceLastMatch', 'HTdaysSinceLastMatch', 'ATdaysSinceLastMatch', 'HTW_rate', 'ATW_rate',\n",
    "                            'ATD_rate', 'HTD_rate', '1_last_HTR_isW', '1_last_HTR_isL', '1_last_ATR_isW', '1_last_ATR_isL']])'''\n",
    "\n",
    "X = pd.get_dummies(E0_data[['HomeTeam', 'AwayTeam', 'HTeamEloScore', 'ATeamEloScore', 'HTdaysSinceLastMatch',\n",
    "                            'ATdaysSinceLastMatch', 'HTW_rate', 'ATW_rate',\n",
    "                            'ATD_rate', 'HTD_rate', \n",
    "                '7_HTW_rate', '12_HTW_rate', '7_ATW_rate', '12_ATW_rate', \n",
    "                '7_HTD_rate', '12_HTD_rate', '7_ATD_rate', '12_ATD_rate',\n",
    "                '7_HTL_rate', '12_HTL_rate', '7_ATL_rate', '12_ATL_rate',\n",
    "                '5_HTHW_rate', '5_ATAW_rate']])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Y = E0_data[['FTR']].to_numpy().ravel()\n",
    "E0_data.to_csv(\"./input/input.csv\")\n",
    "\n",
    "\n",
    "#XY preprocessing\n",
    "imputer = SimpleImputer()\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "#pd.DataFrame(X_imputed).to_csv(\"./input/datasetImputed.csv\")\n",
    "Y = one_hot_y(Y)\n",
    "\n",
    "\n",
    "#Split X and Y into training and Test Sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_imputed, Y, shuffle=False, test_size=0.2727272727)\n",
    "#pd.DataFrame(x_train).to_csv(\"./input/x_train.csv\")\n",
    "print(x_train.shape)\n",
    "#pd.DataFrame(y_train).to_csv(\"./input/y_train.csv\")\n",
    "\n",
    "#Setup XY to have 10 game steps\n",
    "x_train = time_step(x_train,trn_ssn)\n",
    "y_train = time_step(y_train,trn_ssn)\n",
    "y_train = np.moveaxis(y_train, 0, 1)\n",
    "x_test = time_step(x_test,tst_ssn)\n",
    "y_test = time_step(y_test,tst_ssn)\n",
    "y_test = np.moveaxis(y_test, 0, 1)\n",
    "\n",
    "\n",
    "Tx= x_train.shape[1] #Time steps\n",
    "Ty= y_train.shape[0] #Time Steps\n",
    "num_features = x_train.shape[2] #Features per step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create and Setup Model\n",
    "fbmodel = tf.keras.Sequential()\n",
    "inputs = tf.keras.Input(shape=(Tx, num_features))\n",
    "outputs = []\n",
    "for t in range(Ty):\n",
    "    x = tf.keras.layers.Lambda(lambda z: inputs[:, t,:])(inputs)\n",
    "\n",
    "    x = tf.keras.layers.Reshape((1, num_features))(x)\n",
    "    \n",
    "    x = tf.keras.layers.LSTM(units=16, kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-1))(x)\n",
    "\n",
    "    x = tf.keras.layers.Dropout(rate=0.8)(x)\n",
    "    \n",
    "    out = tf.keras.layers.Dense(3, activation='softmax')(x)\n",
    "    \n",
    "    outputs.append(out)\n",
    "    \n",
    "fbmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "fbmodel.summary()\n",
    "\n",
    "fbmodel.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001)\n",
    "#    ,metrics=[tf.keras.metrics.Accuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Model\n",
    "history = fbmodel.fit(\n",
    "    x_train, list(y_train),\n",
    "    epochs=10000,\n",
    "    batch_size=64,\n",
    "    #validation_split=0.272727,\n",
    "    #show = epoch%100==0,\n",
    "    verbose=1,\n",
    "    shuffle=False,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Metrics Data Setup\n",
    "y_pred = fbmodel.predict(x_test)\n",
    "y_predm = np.asarray(y_pred)\n",
    "y_predm = np.argmax(y_predm, axis=2)\n",
    "y_testm = np.argmax(y_test, axis = 2)\n",
    "\n",
    "y_pred_train = fbmodel.predict(x_train)\n",
    "y_pred_train = np.asarray(y_pred_train)\n",
    "y_predm_train = np.argmax(y_pred_train, axis=2)\n",
    "y_trainm = np.argmax(y_train, axis = 2)\n",
    "\n",
    "y_predm = revert_yoh(y_predm).ravel()\n",
    "y_testm = revert_yoh(y_testm).ravel()\n",
    "\n",
    "y_predm_train = revert_yoh(y_predm_train).ravel()\n",
    "y_trainm = revert_yoh(y_trainm).ravel()\n",
    "\n",
    "#Model Metrics\n",
    "print('Train Score: ', accuracy_score(y_trainm,y_predm_train))\n",
    "print('Test Score: ', accuracy_score(y_testm, y_predm))\n",
    "print(classification_report(y_testm, y_predm, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:52:10) \n[Clang 14.0.6 ]"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "e35c5856cff72d20e6e0e19445c0d45b03888a7b19367444be448131dd51e693"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
