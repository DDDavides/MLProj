{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#import needed Python libraries\n",
    "import numpy as np\n",
    "import csv\n",
    "import random\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as scistats\n",
    "import math\n",
    "import pylab\n",
    "import statsmodels as sm\n",
    "\n",
    "#graphics parameters of the notebook\n",
    "# display graphs inline\n",
    "%matplotlib inline\n",
    "\n",
    "# Make graphs prettier\n",
    "pd.set_option('display.max_columns', 15)\n",
    "pd.set_option('display.width', 400)\n",
    "pd.set_option('plotting.matplotlib.register_converters', True)\n",
    "\n",
    "# Make the fonts bigger\n",
    "plt.rc('figure', figsize=(14, 7))\n",
    "plt.rc('font', family='normal', weight='bold', size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#inegrate data from 2009-2010 to 2018-2019 seasons from different files\n",
    "data_18_19 = pd.read_csv(\"./data/2018_2019.csv\", parse_dates=True)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
    "data_17_18 = pd.read_csv(\"./data/2017_2018.csv\", parse_dates=True)\n",
    "data_16_17 = pd.read_csv(\"./data/2016_2017.csv\", parse_dates=True)\n",
    "data_15_16 = pd.read_csv(\"./data/2015_2016.csv\", parse_dates=True)\n",
    "data_14_15 = pd.read_csv(\"./data/2014_2015.csv\", parse_dates=True)\n",
    "data_13_14 = pd.read_csv(\"./data/2013_2014.csv\", parse_dates=True)\n",
    "data_12_13 = pd.read_csv(\"./data/2012_2013.csv\", parse_dates=True)\n",
    "data_11_12 = pd.read_csv(\"./data/2011_2012.csv\", parse_dates=True)\n",
    "data_10_11 = pd.read_csv(\"./data/2010_2011.csv\", parse_dates=True)\n",
    "data_09_10 = pd.read_csv(\"./data/2009_2010.csv\", parse_dates=True)\n",
    "data_08_09 = pd.read_csv(\"./data/2008_2009.csv\", parse_dates=True)\n",
    "#data_07_08 = pd.read_csv(\"./data/2007_2008.csv\", parse_dates=True)\n",
    "#data_06_07 = pd.read_csv(\"./data/2006_2007.csv\", parse_dates=True)\n",
    "#data_05_06 = pd.read_csv(\"./data/2005_2006.csv\", parse_dates=True)\n",
    "\n",
    "\n",
    "\n",
    "#test about data consistency for all files\n",
    "for df in [data_18_19, data_17_18, data_16_17, data_15_16, data_14_15, data_13_14, data_12_13, data_11_12, data_10_11, data_09_10, data_08_09]:\n",
    "    print(\"Number of df columns : \" + str(len(data_18_19.columns)))\n",
    "\n",
    "#integrate data in a single df\n",
    "raw_data = pd.concat([data_18_19, data_17_18, data_16_17, data_15_16, data_14_15, data_13_14, data_12_13, data_11_12, data_10_11, data_09_10, data_08_09])\n",
    "\n",
    "print(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Select useful features for datavisualization and analysis purposes\n",
    "E0_data = raw_data[[\"Date\", \"HomeTeam\", \"AwayTeam\", \"FTHG\", \"FTAG\",\n",
    "                    \"FTR\", \"HTAG\", 'B365A', 'B365D', 'B365H', 'BSA', \n",
    "                    'BSD', 'BSH', 'BWA', 'BWD', 'BWH', 'GBA', 'GBD',\n",
    "                    'GBH', 'IWA', 'IWD', 'IWH', 'LBA', 'LBD', 'LBH',\n",
    "                    'PSA', 'PSD', 'PSH', 'SBA', 'SBD', 'SBH', 'SJA',\n",
    "                    'SJD', 'SJH', 'VCA', 'VCD', 'VCH', 'WHA','WHD', 'WHH']]\n",
    "\n",
    "#convert date format to YYYY-MM-DD classic format\n",
    "E0_data.Date = E0_data.Date.map(lambda x : \"20\" + x[6:8] + \"-\" + x[3:5] + \"-\" + x[0:2])\n",
    "\n",
    "#sort data by date\n",
    "E0_data.sort_values('Date', inplace=True)\n",
    "\n",
    "#reset data indexes\n",
    "E0_data = E0_data.reset_index(drop=True)\n",
    "\n",
    "#create matchID column\n",
    "E0_data['matchID'] = E0_data.index\n",
    "\n",
    "#create season feature\n",
    "E0_data['Season'] = 0\n",
    "E0_data.Season = E0_data.Date.map(lambda x : int(x[0:4]) if int(x[5:7]) > 6 else int(x[0:4]) - 1)\n",
    "\n",
    "#null values test\n",
    "E0_data.isnull().any()\n",
    "\n",
    "#create teams list\n",
    "teams = E0_data['HomeTeam'].unique()\n",
    "print(teams)\n",
    "\n",
    "#create seasons list\n",
    "seasons = np.sort(E0_data['Season'].unique())\n",
    "print(seasons)\n",
    "\n",
    "#create dictionary containing teams list by season\n",
    "teams_by_season = {season : E0_data[E0_data['Season'] == season]['HomeTeam'].unique() for season in seasons}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#match day feature construction for HomeTeam and AwayTeam (1st match of a season --> 1, last --> 38 because 20 team play by season)\n",
    "E0_HT_grpby = E0_data.groupby('HomeTeam')[['Date']]\n",
    "E0_AT_grpby = E0_data.groupby('AwayTeam')[['Date']]\n",
    "\n",
    "def fxyH(row):\n",
    "    x = row['HomeTeam']\n",
    "    y = row['Date']\n",
    "    df1 = E0_HT_grpby.get_group(x)\n",
    "    df2 = E0_AT_grpby.get_group(x)\n",
    "    df1 = df1[df1['Date'] < y]\n",
    "    df2 = df2[df2['Date'] < y]\n",
    "    day = (1 + len(df1) + len(df2)) % 38\n",
    "    return 38 if day == 0 else day \n",
    "\n",
    "def fxyA(row):\n",
    "    x = row['AwayTeam']\n",
    "    y = row['Date']\n",
    "    df1 = E0_HT_grpby.get_group(x)\n",
    "    df2 = E0_AT_grpby.get_group(x)\n",
    "    df1 = df1[df1['Date'] < y]\n",
    "    df2 = df2[df2['Date'] < y]\n",
    "    day = (1 + len(df1) + len(df2)) % 38\n",
    "    return 38 if day == 0 else day \n",
    "\n",
    "E0_data['HomeTeamDay'] = E0_data.apply(fxyH, axis=1)\n",
    "E0_data['AwayTeamDay'] = E0_data.apply(fxyA, axis=1)\n",
    "\n",
    "E0_data['ones'] = 1\n",
    "for season in seasons:\n",
    "    for team in teams_by_season[season]:\n",
    "        sH = E0_data[(E0_data['HomeTeam'] == team) & (E0_data['Season'] == season)]['ones']\n",
    "        E0_data.loc[sH.index, 'HomeTeamHomeDay'] = sH.cumsum()\n",
    "        \n",
    "        sA = E0_data[(E0_data['AwayTeam'] == team) & (E0_data['Season'] == season)]['ones']\n",
    "        E0_data.loc[sA.index, 'AwayTeamAwayDay'] = sA.cumsum()\n",
    "        \n",
    "        \n",
    "def resultConverter(A):\n",
    "    if A == 'H':\n",
    "        return 'W'\n",
    "    elif A =='A':\n",
    "        return 'L'\n",
    "    else:\n",
    "        return 'D'\n",
    "\n",
    "def resultInverser(A):\n",
    "    if A == 'W':\n",
    "        return 'L'\n",
    "    elif A == 'L':\n",
    "        return 'W'\n",
    "    else:\n",
    "        return 'D'\n",
    "def ordinalResultConverter(A):\n",
    "    if A == 'W':\n",
    "        return 1\n",
    "    elif A == 'L':\n",
    "        return 0\n",
    "    else:\n",
    "        return 0.5\n",
    "    \n",
    "#make dummies variables for FTR (result of match), HW = Home Win, AW = Away Win, D = draw\n",
    "E0_data['HW'] = E0_data.FTR.map(lambda x : 1 if x == 'H' else 0)\n",
    "E0_data['AW'] = E0_data.FTR.map(lambda x : 1 if x == 'A' else 0)\n",
    "E0_data['D']= E0_data.FTR.map(lambda x : 1 if x == 'D' else 0)\n",
    "\n",
    "#make 2 different variable for the result of a match : 1 for the home team point of view, the other for the away team pt of view\n",
    "E0_data['HR'] = E0_data.FTR.map(lambda x : resultConverter(x))\n",
    "E0_data['AR'] = E0_data.HR.map(lambda x : resultInverser(x))\n",
    "\n",
    "#make ordinal variable for the home team point of view result (1 = win, 0.5 = Draw, 0 = loss)\n",
    "E0_data['ordinalHR'] = E0_data.HR.map(lambda x : ordinalResultConverter(x))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "grp_by_HT = E0_data.groupby('HomeTeam')\n",
    "grp_by_AT = E0_data.groupby('AwayTeam')\n",
    "\n",
    "grp_by_HT_and_season = E0_data.groupby(['HomeTeam', 'Season'])\n",
    "grp_by_AT_and_season = E0_data.groupby(['AwayTeam', 'Season'])\n",
    "\n",
    "#past performance features engineering\n",
    "for team in teams:\n",
    "    \n",
    "    #we retrieve results series of the team\n",
    "    teamHomeResults_s = grp_by_HT.get_group(team)['HR']\n",
    "    teamAwayResults_s = grp_by_AT.get_group(team)['AR']\n",
    "    #combine these 2 series and sort the obtained serie\n",
    "    teamResults_s = pd.concat([teamHomeResults_s, teamAwayResults_s]).sort_index()\n",
    "\n",
    "    #(i) compute k_last_HR and k_last_AR --> 6 features\n",
    "    lag1TeamResults_d = teamResults_s.shift(1).to_dict()\n",
    "    lag2TeamResults_d = teamResults_s.shift(2).to_dict()\n",
    "    lag3TeamResults_d = teamResults_s.shift(3).to_dict()\n",
    "    \n",
    "    #k_last_HTR and k_last_ATR are just shifted versions of the results series\n",
    "    E0_data.loc[teamHomeResults_s.index,'1_last_HTR'] = E0_data.loc[teamHomeResults_s.index,:].index.map(lambda x : lag1TeamResults_d[x])\n",
    "    E0_data.loc[teamHomeResults_s.index,'2_last_HTR'] = E0_data.loc[teamHomeResults_s.index,:].index.map(lambda x : lag2TeamResults_d[x])\n",
    "    E0_data.loc[teamHomeResults_s.index,'3_last_HTR'] = E0_data.loc[teamHomeResults_s.index,:].index.map(lambda x : lag3TeamResults_d[x])\n",
    "    E0_data.loc[teamAwayResults_s.index,'1_last_ATR'] = E0_data.loc[teamAwayResults_s.index,:].index.map(lambda x : lag1TeamResults_d[x])\n",
    "    E0_data.loc[teamAwayResults_s.index,'2_last_ATR'] = E0_data.loc[teamAwayResults_s.index,:].index.map(lambda x : lag2TeamResults_d[x])\n",
    "    E0_data.loc[teamAwayResults_s.index,'3_last_ATR'] = E0_data.loc[teamAwayResults_s.index,:].index.map(lambda x : lag3TeamResults_d[x])\n",
    "    \n",
    "    #(ii) Compute k_last_HTRH and k_last ATAR --> 4 features\n",
    "    #we need here to diferentiate home results and past results. Python dictionaries allows the program to access to\n",
    "    #needed data faster than with a Pandas serie\n",
    "    lag1TeamHomeResults_d = teamHomeResults_s.shift(1).to_dict()\n",
    "    lag2TeamHomeResults_d = teamHomeResults_s.shift(2).to_dict()\n",
    "    lag1TeamAwayResults_d = teamAwayResults_s.shift(1).to_dict()\n",
    "    lag2TeamAwayResults_d = teamAwayResults_s.shift(2).to_dict()\n",
    "    \n",
    "    E0_data.loc[teamHomeResults_s.index,'1_last_HTHR'] = E0_data.loc[teamHomeResults_s.index,:].index.map(lambda x : lag1TeamHomeResults_d[x])\n",
    "    E0_data.loc[teamHomeResults_s.index,'2_last_HTHR'] = E0_data.loc[teamHomeResults_s.index,:].index.map(lambda x : lag2TeamHomeResults_d[x])\n",
    "    E0_data.loc[teamAwayResults_s.index,'1_last_ATAR'] = E0_data.loc[teamAwayResults_s.index,:].index.map(lambda x : lag1TeamAwayResults_d[x])\n",
    "    E0_data.loc[teamAwayResults_s.index,'2_last_ATAR'] = E0_data.loc[teamAwayResults_s.index,:].index.map(lambda x : lag2TeamAwayResults_d[x])\n",
    "    \n",
    "    #(iii) rates based features : we need to get only season specific results series (to avoid taking previous season results into season rates)\n",
    "    for season in seasons:\n",
    "        \n",
    "        if team in teams_by_season[season]:\n",
    "            #retrieve season specific results serie (1 win serie, 1 draw serie the loss  will be computed thanks to\n",
    "            #the 2 others)\n",
    "            teamHomeResultsW_s = grp_by_HT_and_season.get_group((team,season))['HW']\n",
    "            teamAwayResultsW_s = grp_by_AT_and_season.get_group((team,season))['AW']\n",
    "            teamResultsW_s = pd.concat([teamHomeResultsW_s, teamAwayResultsW_s]).sort_index()\n",
    "\n",
    "            teamHomeResultsD_s = grp_by_HT_and_season.get_group((team,season))['D']\n",
    "            teamAwayResultsD_s = grp_by_AT_and_season.get_group((team,season))['D']\n",
    "            teamResultsD_s = pd.concat([teamHomeResultsD_s, teamAwayResultsD_s]).sort_index()\n",
    "        \n",
    "            #(0) compute HW rates, HL rates, AW rates, LW rates since begining of season\n",
    "            teamResultsWCumul_d = teamResultsW_s.shift(1).cumsum().to_dict()\n",
    "            teamResultsDCumul_d = teamResultsD_s.shift(1).cumsum().to_dict()\n",
    "\n",
    "            #(i) compute 7_HTW_rate, 12_HTW_rate, 7_HTD_rate, 12_HTD_rate, 7_ATW_rate, 12_ATW_rate, 7_ATD_rate, 12_ATD_rate --> 8 features\n",
    "            win7TeamResultsW_d = teamResultsW_s.shift(1).rolling(window = 7, min_periods = 5).mean().to_dict()\n",
    "            win12TeamResultsW_d = teamResultsW_s.shift(1).rolling(window = 12, min_periods = 8).mean().to_dict()\n",
    "            win7TeamResultsD_d = teamResultsD_s.shift(1).rolling( window = 7, min_periods = 5).mean().to_dict()\n",
    "            win12TeamResultsD_d = teamResultsD_s.shift(1).rolling( window = 12, min_periods = 8).mean().to_dict()\n",
    "        \n",
    "            E0_data.loc[teamHomeResultsW_s.index,'HTW_rate'] = E0_data.loc[teamHomeResultsW_s.index,:].index.map(lambda x : teamResultsWCumul_d[x])\n",
    "            E0_data.loc[teamAwayResultsW_s.index,'ATW_rate'] = E0_data.loc[teamAwayResultsW_s.index,:].index.map(lambda x : teamResultsWCumul_d[x])\n",
    "            E0_data.loc[teamHomeResultsW_s.index,'HTD_rate'] = E0_data.loc[teamHomeResultsW_s.index,:].index.map(lambda x : teamResultsDCumul_d[x])\n",
    "            E0_data.loc[teamAwayResultsW_s.index,'ATD_rate'] = E0_data.loc[teamAwayResultsW_s.index,:].index.map(lambda x : teamResultsDCumul_d[x])\n",
    "        \n",
    "            E0_data.loc[teamHomeResultsW_s.index,'7_HTW_rate'] = E0_data.loc[teamHomeResultsW_s.index,:].index.map(lambda x : win7TeamResultsW_d[x])\n",
    "            E0_data.loc[teamHomeResultsW_s.index,'12_HTW_rate'] = E0_data.loc[teamHomeResultsW_s.index,:].index.map(lambda x : win12TeamResultsW_d[x])\n",
    "            E0_data.loc[teamAwayResultsW_s.index,'7_ATW_rate'] = E0_data.loc[teamAwayResultsW_s.index,:].index.map(lambda x : win7TeamResultsW_d[x])\n",
    "            E0_data.loc[teamAwayResultsW_s.index,'12_ATW_rate'] = E0_data.loc[teamAwayResultsW_s.index,:].index.map(lambda x : win12TeamResultsW_d[x])\n",
    "        \n",
    "            E0_data.loc[teamHomeResultsD_s.index,'7_HTD_rate'] = E0_data.loc[teamHomeResultsD_s.index,:].index.map(lambda x : win7TeamResultsD_d[x])\n",
    "            E0_data.loc[teamHomeResultsD_s.index,'12_HTD_rate'] = E0_data.loc[teamHomeResultsD_s.index,:].index.map(lambda x : win12TeamResultsD_d[x])\n",
    "            E0_data.loc[teamAwayResultsD_s.index,'7_ATD_rate'] = E0_data.loc[teamAwayResultsD_s.index,:].index.map(lambda x : win7TeamResultsD_d[x])\n",
    "            E0_data.loc[teamAwayResultsD_s.index,'12_ATD_rate'] = E0_data.loc[teamAwayResultsD_s.index,:].index.map(lambda x : win12TeamResultsD_d[x])\n",
    "\n",
    "        #(ii) compute 5_HTHW_rate and 5_ATAW_rate\n",
    "        win5TeamResultsHomeW_d = teamHomeResultsW_s.shift(1).rolling( window = 5, min_periods = 3).mean().to_dict()\n",
    "        win5TeamResultsAwayW_d = teamAwayResultsW_s.shift(1).rolling( window = 5, min_periods = 3).mean().to_dict()\n",
    "        E0_data.loc[teamHomeResultsW_s.index,'5_HTHW_rate'] = E0_data.loc[teamHomeResultsW_s.index,:].index.map(lambda x : win5TeamResultsHomeW_d[x])\n",
    "        E0_data.loc[teamAwayResultsW_s.index,'5_ATAW_rate'] = E0_data.loc[teamAwayResultsW_s.index,:].index.map(lambda x : win5TeamResultsAwayW_d[x])\n",
    "        \n",
    "        #(iii) compute HTHW_rate, ATAW_rate, HTHD_rate, ATAD_rate\n",
    "        teamHomeResultsCumulW_d = teamHomeResultsW_s.shift(1).cumsum().to_dict()\n",
    "        teamHomeResultsCumulD_d = teamHomeResultsD_s.shift(1).cumsum().to_dict()\n",
    "        teamAwayResultsCumulW_d = teamAwayResultsW_s.shift(1).cumsum().to_dict()\n",
    "        teamAwayResultsCumulD_d = teamAwayResultsD_s.shift(1).cumsum().to_dict()\n",
    "        E0_data.loc[teamHomeResultsW_s.index,'HTHW_rate'] = E0_data.loc[teamHomeResultsW_s.index,:].index.map(lambda x : teamHomeResultsCumulW_d[x])\n",
    "        E0_data.loc[teamHomeResultsW_s.index,'HTHD_rate'] = E0_data.loc[teamHomeResultsW_s.index,:].index.map(lambda x : teamHomeResultsCumulD_d[x])\n",
    "        E0_data.loc[teamAwayResultsW_s.index,'ATAW_rate'] = E0_data.loc[teamAwayResultsW_s.index,:].index.map(lambda x : teamAwayResultsCumulW_d[x])\n",
    "        E0_data.loc[teamAwayResultsW_s.index,'ATAD_rate'] = E0_data.loc[teamAwayResultsW_s.index,:].index.map(lambda x : teamAwayResultsCumulD_d[x])\n",
    "\n",
    "\n",
    "        \n",
    "#compute missing features k_XTL_rate thanks to the k_XTW_rate and k_XTD_rate features\n",
    "E0_data.loc[:,'7_HTL_rate'] = 1 - (E0_data['7_HTW_rate'] + E0_data['7_HTD_rate'])\n",
    "E0_data.loc[:,'12_HTL_rate'] = 1 - (E0_data['7_HTW_rate'] + E0_data['7_HTD_rate'])\n",
    "E0_data.loc[:,'7_ATL_rate'] = 1 - (E0_data['7_ATW_rate'] + E0_data['7_ATD_rate'])\n",
    "E0_data.loc[:,'12_ATL_rate'] = 1 - (E0_data['7_ATW_rate'] + E0_data['7_ATD_rate'])\n",
    "\n",
    "#compute missing HTL_rate, ATL_rate with features with the wins and draws features\n",
    "E0_data.loc[:,'HTW_rate'] = E0_data['HTW_rate']/E0_data['HomeTeamDay']\n",
    "E0_data.loc[:,'ATW_rate'] = E0_data['ATW_rate']/E0_data['AwayTeamDay']\n",
    "E0_data.loc[:,'HTD_rate'] = E0_data['HTD_rate']/E0_data['HomeTeamDay']\n",
    "E0_data.loc[:,'ATD_rate'] = E0_data['ATD_rate']/E0_data['AwayTeamDay']\n",
    "E0_data.loc[:,'HTL_rate'] = 1 - (E0_data['HTW_rate'] + E0_data['HTD_rate'])\n",
    "E0_data.loc[:,'ATL_rate'] = 1 - (E0_data['ATW_rate'] + E0_data['ATD_rate'])\n",
    "\n",
    "#we finish to compute HTHW_rate, ..., ATAD_rate features and compute corresponding loss features\n",
    "E0_data.loc[:,'HTHW_rate'] = E0_data['HTHW_rate']/E0_data['HomeTeamHomeDay']\n",
    "E0_data.loc[:,'ATAW_rate'] = E0_data['ATAW_rate']/E0_data['AwayTeamAwayDay']\n",
    "E0_data.loc[:,'HTHD_rate'] = E0_data['HTHD_rate']/E0_data['HomeTeamHomeDay']\n",
    "E0_data.loc[:,'ATAD_rate'] = E0_data['ATAD_rate']/E0_data['AwayTeamAwayDay']\n",
    "E0_data.loc[:,'HTHL_rate'] = 1 - (E0_data['HTHW_rate'] + E0_data['HTHD_rate'])\n",
    "E0_data.loc[:,'ATAL_rate'] = 1 - (E0_data['ATAW_rate'] + E0_data['ATAD_rate'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Elo ranking method parameters\n",
    "k = 20.0\n",
    "d = 400.0\n",
    "c = 10.0\n",
    "\n",
    "#Initialization of output containers\n",
    "ELO_dict = dict()\n",
    "gammaHT_dict = dict()\n",
    "gammaAT_dict = dict()\n",
    "\n",
    "#intermediate data containers initilization\n",
    "latest_update_d = dict() #contains latest updates in date of ELO_dict\n",
    "prev_ELO_score_d = dict() #contains latest ELO_score given to a team for computing new one\n",
    "\n",
    "prev_season_teams = [team for team in teams] #contains list of teams for the current season\n",
    "last_teams_ELO_av = 0.0 #contains ELO average of last previous season teams\n",
    "\n",
    "for team in teams:\n",
    "    latest_update_d[team] = '2001-01-01'\n",
    "    prev_ELO_score_d[team] = 0.0\n",
    "\n",
    "for season in E0_data.Season.unique():\n",
    "    season_match_dates = E0_data[E0_data['Season'] == season].Date.unique()\n",
    "    season_teams = E0_data[E0_data['Season'] == season].HomeTeam.unique()\n",
    "    last_season_date = season_match_dates[len(season_match_dates) - 1]\n",
    "    \n",
    "    for Steam in season_teams:\n",
    "        if not (Steam in prev_season_teams):\n",
    "            prev_ELO_score_d[Steam] = last_teams_ELO_av\n",
    "            \n",
    "    for date in season_match_dates:\n",
    "        for team in teams:\n",
    "            if not ((team in E0_data[E0_data['Date'] == date]['HomeTeam'].values) | (team in E0_data[E0_data['Date'] == date]['AwayTeam'].values)):\n",
    "                ELO_dict[(team, date)] = prev_ELO_score_d[team]\n",
    "                latest_update_d[team] = date\n",
    "            else:\n",
    "                if latest_update_d[team] < date:\n",
    "                    Hteam = E0_data[(E0_data['Date'] == date) & ((E0_data['HomeTeam'] == team) | (E0_data['AwayTeam'] == team))]['HomeTeam'].values[0]\n",
    "                    Ateam = E0_data[(E0_data['Date'] == date) & ((E0_data['HomeTeam'] == team) | (E0_data['AwayTeam'] == team))]['AwayTeam'].values[0]\n",
    "            \n",
    "                    l0H = prev_ELO_score_d[Hteam]\n",
    "                    l0A = prev_ELO_score_d[Ateam]\n",
    "                    gammaH = 1.0/(1.0 + c**((l0A - l0H)/d))\n",
    "                    gammaA = 1.0 - gammaH\n",
    "                    alphaH = E0_data[(E0_data['Date'] == date) & (E0_data['HomeTeam'] == Hteam)]['ordinalHR'].values[0]\n",
    "                    alphaA = 1 - alphaH\n",
    "            \n",
    "                    #compute new scores\n",
    "                    new_HT_ELO_score = l0H + k * (alphaH - gammaH)\n",
    "                    new_AT_ELO_score = l0A + k * (alphaA - gammaA)\n",
    "\n",
    "                    #put new scores in ELO_dict\n",
    "                    ELO_dict[(Hteam, date)] = new_HT_ELO_score\n",
    "                    ELO_dict[(Ateam, date)] = new_AT_ELO_score\n",
    "                    gammaHT_dict[(Hteam, date)] = gammaH\n",
    "                    gammaAT_dict[(Ateam, date)] = gammaA\n",
    "                    latest_update_d[Hteam] = date\n",
    "                    latest_update_d[Ateam] = date\n",
    "            \n",
    "                    #update prev_ELO_score_d and latest_update_d\n",
    "                    prev_ELO_score_d[Hteam] = new_HT_ELO_score\n",
    "                    prev_ELO_score_d[Ateam] = new_AT_ELO_score\n",
    "        \n",
    "        if date == last_season_date:\n",
    "            ELOs = np.array([prev_ELO_score_d[Steam] for Steam in season_teams])\n",
    "            ELOs.sort()\n",
    "            last_teams_ELO_av = np.mean(ELOs[0:-17])\n",
    "            prev_season_teams = season_teams\n",
    "            \n",
    "            #make HTeamEloScore, ATeamEloScore and gammaHome features from previously computed dictionaries\n",
    "\n",
    "def HomeTeamEloScore(row):\n",
    "    return ELO_dict[(row['HomeTeam'], row['Date'])]\n",
    "\n",
    "def AwayTeamEloScore(row):\n",
    "    return ELO_dict[(row['AwayTeam'], row['Date'])]\n",
    "\n",
    "def gammaHTeamDate(row):\n",
    "    return gammaHT_dict[(row['HomeTeam'], row['Date'])]\n",
    "\n",
    "#compute resulting Elo scores important features\n",
    "E0_data.loc[:,'HTeamEloScore'] = E0_data.apply(HomeTeamEloScore, axis=1) \n",
    "E0_data.loc[:,'ATeamEloScore'] = E0_data.apply(AwayTeamEloScore, axis=1) \n",
    "E0_data.loc[:,'gammaHome'] = E0_data.apply(gammaHTeamDate, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for team in teams:\n",
    "    homeMatchDates_s = E0_data[E0_data['HomeTeam'] == team]['Date']\n",
    "    awayMatchDates_s = E0_data[E0_data['AwayTeam'] == team]['Date']\n",
    "    matchDates_s = pd.concat([homeMatchDates_s, awayMatchDates_s]).sort_index()\n",
    "    lastMatchDates_s = matchDates_s.shift(1)\n",
    "    matchDates = matchDates_s.values\n",
    "        \n",
    "    E0_data.loc[E0_data['HomeTeam'] == team, 'HTLastMatchDate'] = E0_data.loc[E0_data['HomeTeam'] == team].index.map(lambda x : lastMatchDates_s[x])\n",
    "    E0_data.loc[E0_data['AwayTeam'] == team, 'ATLastMatchDate'] = E0_data.loc[E0_data['AwayTeam'] == team].index.map(lambda x : lastMatchDates_s[x])\n",
    "    \n",
    "def HTdaysBetweenDates(row):\n",
    "    if not (pd.isnull(row['HTLastMatchDate'])):\n",
    "        currDate = pd.to_datetime(row['Date'])\n",
    "        prevDate = pd.to_datetime(row['HTLastMatchDate'])\n",
    "        ndays = (currDate - prevDate).days \n",
    "        if ndays < 20:\n",
    "            return ndays\n",
    "        else: \n",
    "            return np.nan\n",
    "    else:\n",
    "        return np.nan \n",
    "    \n",
    "def ATdaysBetweenDates(row):\n",
    "    if not (pd.isnull(row['ATLastMatchDate'])):\n",
    "        currDate = pd.to_datetime(row['Date'])\n",
    "        prevDate = pd.to_datetime(row['ATLastMatchDate'])\n",
    "        return (currDate - prevDate).days\n",
    "    else:\n",
    "        return np.nan \n",
    "    \n",
    "E0_data.loc[:, 'HTdaysSinceLastMatch'] = E0_data.apply(HTdaysBetweenDates, axis=1)\n",
    "E0_data.loc[:, 'ATdaysSinceLastMatch'] = E0_data.apply(ATdaysBetweenDates, axis=1)\n",
    "E0_data.loc[:,'DaysSinceLastMatchRate'] = E0_data['HTdaysSinceLastMatch'].astype(float)/E0_data['ATdaysSinceLastMatch'].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "E0_data['1_last_HTR_isW'] = E0_data['1_last_HTR'].map(lambda x : 1 if x == 'W' else 0)\n",
    "E0_data['1_last_HTR_isL'] = E0_data['1_last_HTR'].map(lambda x : 1 if x == 'L' else 0) \n",
    "E0_data['2_last_HTR_isW'] = E0_data['2_last_HTR'].map(lambda x : 1 if x == 'W' else 0)\n",
    "E0_data['2_last_HTR_isL'] = E0_data['2_last_HTR'].map(lambda x : 1 if x == 'L' else 0) \n",
    "E0_data['3_last_HTR_isW'] = E0_data['3_last_HTR'].map(lambda x : 1 if x == 'W' else 0)\n",
    "E0_data['3_last_HTR_isL'] = E0_data['3_last_HTR'].map(lambda x : 1 if x == 'L' else 0) \n",
    "\n",
    "E0_data['1_last_ATR_isW'] = E0_data['1_last_ATR'].map(lambda x : 1 if x == 'W' else 0)\n",
    "E0_data['1_last_ATR_isL'] = E0_data['1_last_ATR'].map(lambda x : 1 if x == 'L' else 0) \n",
    "E0_data['2_last_ATR_isW'] = E0_data['2_last_ATR'].map(lambda x : 1 if x == 'W' else 0)\n",
    "E0_data['2_last_ATR_isL'] = E0_data['2_last_ATR'].map(lambda x : 1 if x == 'L' else 0) \n",
    "E0_data['3_last_ATR_isW'] = E0_data['3_last_ATR'].map(lambda x : 1 if x == 'W' else 0)\n",
    "E0_data['3_last_ATR_isL'] = E0_data['3_last_ATR'].map(lambda x : 1 if x == 'L' else 0) \n",
    "\n",
    "E0_data['1_last_HTHR_isW'] = E0_data['1_last_HTHR'].map(lambda x : 1 if x == 'W' else 0)\n",
    "E0_data['1_last_HTHR_isL'] = E0_data['1_last_HTHR'].map(lambda x : 1 if x == 'L' else 0) \n",
    "E0_data['2_last_HTHR_isW'] = E0_data['2_last_HTHR'].map(lambda x : 1 if x == 'W' else 0)\n",
    "E0_data['2_last_HTHR_isL'] = E0_data['2_last_HTHR'].map(lambda x : 1 if x == 'L' else 0)\n",
    "\n",
    "E0_data['1_last_ATAR_isW'] = E0_data['1_last_ATAR'].map(lambda x : 1 if x == 'W' else 0)\n",
    "E0_data['1_last_ATAR_isL'] = E0_data['1_last_ATAR'].map(lambda x : 1 if x == 'L' else 0) \n",
    "E0_data['2_last_ATAR_isW'] = E0_data['2_last_ATAR'].map(lambda x : 1 if x == 'W' else 0)\n",
    "E0_data['2_last_ATAR_isL'] = E0_data['2_last_ATAR'].map(lambda x : 1 if x == 'L' else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Home wins, Away wins and draws rates variations over seasons\n",
    "HW_rates = []\n",
    "AW_rates = []\n",
    "D_rates = []\n",
    "\n",
    "for season in seasons:\n",
    "    season_data = E0_data[E0_data['Season'] == season]\n",
    "    total_matches_nb = len(season_data.index)\n",
    "    HW_rate = float(len(season_data[season_data['FTR'] == 'H'].index))/float(total_matches_nb)\n",
    "    AW_rate = float(len(season_data[season_data['FTR'] == 'A'].index))/float(total_matches_nb)\n",
    "    D_rate = float(len(season_data[season_data['FTR'] == 'D'].index))/float(total_matches_nb)\n",
    "    HW_rates.append(HW_rate)\n",
    "    AW_rates.append(AW_rate)\n",
    "    D_rates.append(D_rate)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(seasons, HW_rates, label=\"Home wins rate\")\n",
    "plt.plot(seasons, AW_rates, label=\"Away wins rate\")\n",
    "plt.plot(seasons, D_rates, label=\"Draw rates\")\n",
    "plt.legend()\n",
    "plt.xticks([int(season) for season in seasons], seasons)\n",
    "plt.xlabel(\"Seasons\")\n",
    "plt.ylabel(\"Home wins, draws and losses rates\")\n",
    "plt.title(\"Home wins, draws and losses rates evolution over seasons\")\n",
    "\n",
    "#global wins, draws and losses rates\n",
    "HW_rate = float(len(E0_data[E0_data['HR'] == 'W'].index))/float(len(E0_data.index))\n",
    "HL_rate = float(len(E0_data[E0_data['HR'] == 'L'].index))/float(len(E0_data.index))\n",
    "HD_rate = float(len(E0_data[E0_data['HR'] == 'D'].index))/float(len(E0_data.index))\n",
    "rates = [HW_rate, HL_rate, HD_rate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "width = 0.12\n",
    "xticks = np.array([0.3, 1.0, 1.7])\n",
    "xticksLabels = ['0.0 - 0.2', '0.2 - 0.4', '0.4 - 0.6']\n",
    "xlabels = [[\"Last home team result = W\", \"Last home team result = D\", \"Last home team result = L\"],\n",
    "           [\"2nd last home team result = W\", \"2nd last home team result = D\", \"2nd last home team result = L\"],\n",
    "           [\"Last away team result = W\", \"Last away team result = D\", \"Last away team result = L\"],\n",
    "            [\"2nd last away team result = W\", \"2nd last away team result = D\", \"2nd last away team result = L\"]]\n",
    "\n",
    "var_names = ['1_last_HTR', '2_last_HTR', '1_last_ATR', '2_last_ATR']\n",
    "titles = [\"Home results repartition vs. lagged performance values (\" + var_name + \")\" for var_name in var_names]\n",
    "\n",
    "k=0\n",
    "for var_name in var_names:\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    HW_rates = []\n",
    "    HD_rates = []\n",
    "    \n",
    "    for Rvalue in ['W','D','L']:\n",
    "        HW_rates.append(len(E0_data[(E0_data[var_name] == Rvalue) & (E0_data['HR'] == 'W')].index) / len(E0_data[E0_data[var_name] == Rvalue].index))\n",
    "        HD_rates.append(len(E0_data[(E0_data[var_name] == Rvalue) & (E0_data['HR'] == 'D')].index) / len(E0_data[E0_data[var_name] == Rvalue].index))\n",
    "    \n",
    "    HL_rates = [1 - (HW_rate + HD_rate) for (HW_rate, HD_rate) in zip(HW_rates, HD_rates)]\n",
    "    \n",
    "    plt.bar(xticks - 1.5 * width, HW_rates, width, color = '#6495ED')\n",
    "    plt.bar(xticks - 0.5 * width, HD_rates, width, color = '#778899')\n",
    "    plt.bar(xticks + 0.5 * width, HL_rates, width, color = '#FF7F50')\n",
    "    plt.plot((0.025, 2.0), (HW_rate, HW_rate), '--', color='#6495ED', linewidth=2)\n",
    "    plt.plot((0.025, 2.0), (HD_rate, HD_rate), '--', color='#778899', linewidth=2)\n",
    "    plt.plot((0.025, 2.0), (HL_rate, HL_rate), '--', color='#FF7F50', linewidth=2)\n",
    "    \n",
    "    pylab.xticks(xticks, xlabels[k])\n",
    "    plt.ylabel(\"Wins, draws and victories rates\")\n",
    "    plt.legend([\"HT Wins rate\", \"HT Draws rate\", \"HT Losses rate\"])\n",
    "    plt.xlim(0.0, 2.0)\n",
    "    plt.title(titles[k], fontsize=17)\n",
    "\n",
    "    k += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%    \n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "bars = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "width = (0.2 - 0.05)/3.0\n",
    "xticks = np.array([0.025, 0.225, 0.425, 0.625, 0.825, 1.025])\n",
    "xticksLabels = ['0.0 - 0.2', '0.2 - 0.4', '0.4 - 0.6', '0.6 - 0.8', '0.8 - 1.0', '1.0']\n",
    "xlabels = [\"Home Team average Wins rate for last 7 days\", \"Home Team average Wins rate for last 12 days\", \n",
    "          \"Away team average Wins rate for last 7 days\", \"Away team average Wins rate for last 12 days\"]    \n",
    "var_names = ['7_HTW_rate', '12_HTW_rate', '7_ATW_rate', '12_ATW_rate']\n",
    "titles = [\"Home results repartition vs. home/away wins rate over past results (\" + var_name + \")\" for var_name in var_names]\n",
    "\n",
    "\n",
    "k=0\n",
    "for var_name in var_names:\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    nonzero_bars = []\n",
    "    nonzero_xticks = []\n",
    "    HW_rates = []\n",
    "    HD_rates = []\n",
    "    \n",
    "    for (bar, xtick) in zip(bars, xticks):\n",
    "        if len(E0_data[(E0_data[var_name] >= bar) & (E0_data[var_name] < (bar + 0.2))].index) > 0:\n",
    "            nonzero_xticks.append(xtick)\n",
    "            HW_rates.append(len(E0_data[(E0_data[var_name] >= bar) & (E0_data[var_name] < (bar + 0.2)) & (E0_data['HR'] == 'W')].index) / len(E0_data[(E0_data[var_name] >= bar) & (E0_data[var_name] < (bar + 0.2))].index))\n",
    "            HD_rates.append(len(E0_data[(E0_data[var_name] >= bar) & (E0_data[var_name] < (bar + 0.2)) & (E0_data['HR'] == 'D')].index) / len(E0_data[(E0_data[var_name] >= bar) & (E0_data[var_name] < (bar + 0.2))].index))\n",
    "    \n",
    "    HL_rates = [1 - (HW_rate + HD_rate) for (HW_rate, HD_rate) in zip(HW_rates, HD_rates)]\n",
    "    \n",
    "    plt.bar(np.array(nonzero_xticks), HW_rates, width, color = '#6495ED')\n",
    "    plt.bar(np.array(nonzero_xticks) + width, HD_rates, width, color = '#778899')\n",
    "    plt.bar(np.array(nonzero_xticks) + 2*width, HL_rates, width, color = '#FF7F50')\n",
    "    plt.plot((0.025, 1.2), (HW_rate, HW_rate), '--', color='#6495ED', linewidth=2)\n",
    "    plt.plot((0.025, 1.2), (HD_rate, HD_rate), '--', color='#778899', linewidth=2)\n",
    "    plt.plot((0.025, 1.2), (HL_rate, HL_rate), '--', color='#FF7F50', linewidth=2)\n",
    "\n",
    "    pylab.xticks([0.1, 0.3, 0.5, 0.7, 0.9, 1.1], xticksLabels)\n",
    "    plt.xlabel(xlabels[k])\n",
    "    plt.ylabel(\"Wins, draws and losses rates\")\n",
    "    plt.legend([\"HT Wins rate\", \"HT Draws rate\", \"HT Losses rate\"])\n",
    "    plt.title(titles[k], fontsize=17)\n",
    "\n",
    "\n",
    "\n",
    "    k += 1    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%    \n"
    }
   },
   "outputs": [],
   "source": [
    "# Elo rank feature visualization\n",
    "\n",
    "plt.hist(E0_data['gammaHome'].values, bins=20)\n",
    "plt.xlabel(\"gammaHome values\")\n",
    "plt.ylabel(\"Bins sizes\")\n",
    "plt.xlim(0,1)\n",
    "plt.title(\"Elo score distribution histogram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "#random teams elo score visualization\n",
    "random_teams = [teams[random.randint(0, len(teams))-1] for k in range(0,5)]\n",
    "for team in random_teams:\n",
    "    HomeEloScores = E0_data[E0_data['HomeTeam'] == team]['HTeamEloScore']\n",
    "    AwayEloScores = E0_data[E0_data['AwayTeam'] == team]['ATeamEloScore']\n",
    "    EloScores = pd.concat([HomeEloScores, AwayEloScores]).sort_index()\n",
    "    EloScores.plot()\n",
    "plt.legend(random_teams)\n",
    "plt.xlabel('Match number')\n",
    "plt.ylabel('Elo scores')\n",
    "plt.title('Visualization of Elo scores evolution over seasons for 5 random teams', fontsize = 16)\n",
    "    \n",
    "fig = plt.figure()\n",
    "for team in ['Arsenal', 'Man United', 'Liverpool']:\n",
    "    HomeEloScores = E0_data[E0_data['HomeTeam'] == team]['HTeamEloScore']\n",
    "    AwayEloScores = E0_data[E0_data['AwayTeam'] == team]['ATeamEloScore']\n",
    "    EloScores = pd.concat([HomeEloScores, AwayEloScores]).sort_index()\n",
    "    EloScores.plot()\n",
    "plt.legend(['Arsenal', 'Man United', 'Liverpool'])\n",
    "plt.xlabel('Match number')\n",
    "plt.ylabel('Elo scores')\n",
    "plt.title('Visualization of Elo scores evolution over seasons for Arsenal, Man United and Liverpool', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#we define bins of approximatively equal sizes in terms of Elo score\n",
    "bars = [0.0, 0.266, 0.35, 0.43, 0.48, 0.52, 0.62, 0.71, 1.0]\n",
    "bars_pos = [0.0 + k * (1.0/8.0) for k in range(1,9)]\n",
    "width = 0.02\n",
    "xticks = np.array([(bars[k+1] - bars[k])/2.0 for k in range(0, len(bars) - 1)])\n",
    "xticksLabels = ['0.0 - 0.266', '0.266 - 0.35','0.35 - 0.43', '0.43-0.48', '0.48 - 0.52','0.52-0.62', '0.62 - 0.71', '0.71 - 1.0']\n",
    "xlabels = [\"Home Team average home-wins rate for last 5 days (at home)\", \"Away Team average away-wins rate for last 5 days (away)\"]    \n",
    "\n",
    "HW_rates = []\n",
    "HD_rates = []\n",
    "HL_rates = []\n",
    "\n",
    "for k in range(0 , len(bars) - 1):\n",
    "    HW_rates.append(len(E0_data[(E0_data['gammaHome'] >= bars[k]) & (E0_data['gammaHome'] < bars[k+1]) & (E0_data['HR'] == 'W')].index) / len(E0_data[(E0_data['gammaHome'] >= bars[k]) & (E0_data['gammaHome'] < bars[k+1])].index))\n",
    "    HD_rates.append(len(E0_data[(E0_data['gammaHome'] >= bars[k]) & (E0_data['gammaHome'] < bars[k+1]) & (E0_data['HR'] == 'D')].index) / len(E0_data[(E0_data['gammaHome'] >= bars[k]) & (E0_data['gammaHome'] < bars[k+1])].index))\n",
    "    HL_rates = [1 - (HW_rate + HD_rate) for (HW_rate, HD_rate) in zip(HW_rates, HD_rates)]\n",
    "    \n",
    "plt.bar(np.array(bars_pos), HW_rates, width, color = '#6495ED', alpha = 0.85)\n",
    "plt.bar(np.array(bars_pos) + width, HD_rates, width, color = '#778899', alpha = 0.85)\n",
    "plt.bar(np.array(bars_pos) + 2*width, HL_rates, width, color = '#FF7F50', alpha = 0.85)\n",
    "\n",
    "plt.plot((0.0, 1.2), (HW_rate, HW_rate), '--', color='#6495ED', linewidth=2)\n",
    "plt.plot((0.0, 1.2), (HD_rate, HD_rate), '--', color='#778899', linewidth=2)\n",
    "plt.plot((0.0, 1.2), (HL_rate, HL_rate), '--', color='#FF7F50', linewidth=2)\n",
    "    \n",
    "plt.ylabel(\"Wins, draws and losses rates\")\n",
    "plt.legend([\"HT Wins rate\", \"HT Draws rate\", \"HT Losses rate\"])\n",
    "plt.xlim(0,1.2)\n",
    "pylab.xticks(np.array(bars_pos) + width, xticksLabels, rotation=45)\n",
    "plt.title(\"Effect of gammaHome score match home results repartition\", fontsize=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "#we define bins of minimum size for HTdaysSinceLastMatch\n",
    "bins = [0, 3, 4, 5, 6, 7, 8, 9, 11, 14, 15, 21]\n",
    "bars_pos = [0.0 + k * (20/12.0) for k in range(1,12)]\n",
    "width = 0.25\n",
    "xticksLabels = ['0 - 2', '3', '4', '5', '6', '7','8', '9-10', '11-13', '14', '15-20']\n",
    "\n",
    "HW_rates = []\n",
    "HD_rates = []\n",
    "HL_rates = []\n",
    "\n",
    "for k in range(0 , len(bins) - 1):\n",
    "    HW_rates.append(len(E0_data[(E0_data['HTdaysSinceLastMatch'] >= bins[k]) & (E0_data['HTdaysSinceLastMatch'] < bins[k+1]) & (E0_data['HR'] == 'W')].index) / len(E0_data[(E0_data['HTdaysSinceLastMatch'] >= bins[k]) & (E0_data['HTdaysSinceLastMatch'] < bins[k+1])].index))\n",
    "    HD_rates.append(len(E0_data[(E0_data['HTdaysSinceLastMatch'] >= bins[k]) & (E0_data['HTdaysSinceLastMatch'] < bins[k+1]) & (E0_data['HR'] == 'D')].index) / len(E0_data[(E0_data['HTdaysSinceLastMatch'] >= bins[k]) & (E0_data['HTdaysSinceLastMatch'] < bins[k+1])].index))\n",
    "    HL_rates = [1 - (HW_rate + HD_rate) for (HW_rate, HD_rate) in zip(HW_rates, HD_rates)]\n",
    "    \n",
    "plt.bar(np.array(bars_pos), HW_rates, width, color = '#6495ED')\n",
    "plt.bar(np.array(bars_pos) + width, HD_rates, width, color = '#778899')\n",
    "plt.bar(np.array(bars_pos) + 2*width, HL_rates, width, color = '#FF7F50')\n",
    "\n",
    "plt.plot((0.0, 21), (HW_rate, HW_rate), '--', color='#6495ED', linewidth=2)\n",
    "plt.plot((0.0, 21), (HD_rate, HD_rate), '--', color='#778899', linewidth=2)\n",
    "plt.plot((0.0, 21), (HL_rate, HL_rate), '--', color='#FF7F50', linewidth=2)\n",
    "\n",
    "pylab.xticks(np.array(bars_pos) + width, xticksLabels, rotation=45)\n",
    "plt.ylabel(\"Wins, draws and losses rates\")\n",
    "plt.legend([\"HT Wins rate\", \"HT Draws rate\", \"HT Losses rate\"])\n",
    "plt.xlim(0,21)\n",
    "plt.title(\"Influence of HTdaysSinceLastMatch over results repartition\", fontsize = 17)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "E0_data_copy = E0_data[['HW', 'AW', 'D',\n",
    "                        'gammaHome',\n",
    "                        'DaysSinceLastMatchRate',\n",
    "                        'HTW_rate', 'HTD_rate', 'HTL_rate',\n",
    "                        'ATW_rate', 'ATD_rate', 'ATL_rate',\n",
    "                        'HTHW_rate', 'HTHD_rate', 'HTHL_rate',\n",
    "                        'ATAW_rate', 'ATAD_rate', 'ATAL_rate',\n",
    "                        '1_last_HTR', '2_last_HTR',\n",
    "                        '1_last_ATR', '2_last_ATR',\n",
    "                        '1_last_HTHR', '2_last_HTHR',\n",
    "                        '1_last_ATAR', '2_last_ATAR',\n",
    "                        '7_HTW_rate', '12_HTW_rate', \n",
    "                        '7_ATW_rate', '12_ATW_rate', \n",
    "                        '7_HTL_rate', '12_HTL_rate', \n",
    "                        '7_ATL_rate', '12_ATD_rate',\n",
    "                        '7_HTD_rate', '12_HTD_rate',\n",
    "                        '7_ATD_rate', '7_ATD_rate',\n",
    "                       '5_HTHW_rate', '5_ATAW_rate']]\n",
    "\n",
    "#make dummies variables for the past results indicators features (3 for W, D and L)\n",
    "E0_data_copy.loc[:, '1_last_HTRisW'] = E0_data_copy['1_last_HTR'].map(lambda x : 1 if x == 'W' else 0)\n",
    "E0_data_copy.loc[:, '1_last_HTRisD'] = E0_data_copy['1_last_HTR'].map(lambda x : 1 if x == 'D' else 0)\n",
    "E0_data_copy.loc[:, '1_last_HTRisL'] = E0_data_copy['1_last_HTR'].map(lambda x : 1 if x == 'L' else 0)\n",
    "E0_data_copy.loc[:, '2_last_HTRisW'] = E0_data_copy['2_last_HTR'].map(lambda x : 1 if x == 'W' else 0)\n",
    "E0_data_copy.loc[:, '2_last_HTRisD'] = E0_data_copy['2_last_HTR'].map(lambda x : 1 if x == 'D' else 0)\n",
    "E0_data_copy.loc[:, '2_last_HTRisL'] = E0_data_copy['2_last_HTR'].map(lambda x : 1 if x == 'L' else 0)\n",
    "E0_data_copy.loc[:, '1_last_ATRisW'] = E0_data_copy['1_last_ATR'].map(lambda x : 1 if x == 'W' else 0)\n",
    "E0_data_copy.loc[:, '1_last_ATRisD'] = E0_data_copy['1_last_ATR'].map(lambda x : 1 if x == 'D' else 0)\n",
    "E0_data_copy.loc[:, '1_last_ATRisL'] = E0_data_copy['1_last_ATR'].map(lambda x : 1 if x == 'L' else 0)\n",
    "E0_data_copy.loc[:, '2_last_ATRisW'] = E0_data_copy['2_last_ATR'].map(lambda x : 1 if x == 'W' else 0)\n",
    "E0_data_copy.loc[:, '2_last_ATRisD'] = E0_data_copy['2_last_ATR'].map(lambda x : 1 if x == 'D' else 0)\n",
    "E0_data_copy.loc[:, '2_last_ATRisL'] = E0_data_copy['2_last_ATR'].map(lambda x : 1 if x == 'L' else 0)\n",
    "\n",
    "E0_data_copy.loc[:, '1_last_HTHRisW'] = E0_data_copy['1_last_HTHR'].map(lambda x : 1 if x == 'W' else 0)\n",
    "E0_data_copy.loc[:, '1_last_HTHRisD'] = E0_data_copy['1_last_HTHR'].map(lambda x : 1 if x == 'D' else 0)\n",
    "E0_data_copy.loc[:, '1_last_HTHRisL'] = E0_data_copy['1_last_HTHR'].map(lambda x : 1 if x == 'L' else 0)\n",
    "E0_data_copy.loc[:, '2_last_HTHRisW'] = E0_data_copy['2_last_HTHR'].map(lambda x : 1 if x == 'W' else 0)\n",
    "E0_data_copy.loc[:, '2_last_HTHRisD'] = E0_data_copy['2_last_HTHR'].map(lambda x : 1 if x == 'D' else 0)\n",
    "E0_data_copy.loc[:, '2_last_HTHRisL'] = E0_data_copy['2_last_HTHR'].map(lambda x : 1 if x == 'L' else 0)\n",
    "E0_data_copy.loc[:, '1_last_ATARisW'] = E0_data_copy['1_last_ATAR'].map(lambda x : 1 if x == 'W' else 0)\n",
    "E0_data_copy.loc[:, '1_last_ATARisD'] = E0_data_copy['1_last_ATAR'].map(lambda x : 1 if x == 'D' else 0)\n",
    "E0_data_copy.loc[:, '1_last_ATARisL'] = E0_data_copy['1_last_ATAR'].map(lambda x : 1 if x == 'L' else 0)\n",
    "E0_data_copy.loc[:, '2_last_ATARisW'] = E0_data_copy['2_last_ATAR'].map(lambda x : 1 if x == 'W' else 0)\n",
    "E0_data_copy.loc[:, '2_last_ATARisD'] = E0_data_copy['2_last_ATAR'].map(lambda x : 1 if x == 'D' else 0)\n",
    "E0_data_copy.loc[:, '2_last_ATARisL'] = E0_data_copy['2_last_ATAR'].map(lambda x : 1 if x == 'L' else 0)\n",
    "\n",
    "#under matrix form\n",
    "corr = E0_data_copy.corr()\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#and in a more understandable and graphical form\n",
    "def plot_corr(df,size=10):\n",
    "    corr = df.corr()\n",
    "    fig, ax = plt.subplots(figsize=(size, size))\n",
    "    ax.matshow(corr)\n",
    "    plt.xticks(range(len(corr.columns)), corr.columns, rotation = 90);\n",
    "    plt.yticks(range(len(corr.columns)), corr.columns);\n",
    "    \n",
    "plot_corr(E0_data_copy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "eloSeasons = [2005, 2006, 2007, 2008]\n",
    "trainingSeasons = [2009, 2010, 2011, 2012, 2013, 2014, 2015]\n",
    "testSeasons = [2016, 2017,2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Folds generations custom function \n",
    "\n",
    "\n",
    "def foldsGenerator(ixSet, foldMinSize, foldMaxSize, trInitSize, trOptimalSize = -1):\n",
    "    \n",
    "    subsetsList = []\n",
    "    subsetsList.append(ixSet[0:trInitSize])\n",
    "    Nsubsets = 1\n",
    "    \n",
    "    ixSetLength = ixSet.size\n",
    "    \n",
    "    unfoldedSetSize = ixSetLength - trInitSize\n",
    "    prevSubsetStop = trInitSize\n",
    "    \n",
    "    while (unfoldedSetSize > foldMaxSize):\n",
    "        nextFoldSize = random.randint(foldMinSize, foldMaxSize)\n",
    "        \n",
    "        subsetsList.append(ixSet[prevSubsetStop:(prevSubsetStop + nextFoldSize)])\n",
    "        \n",
    "        unfoldedSetSize -= nextFoldSize\n",
    "        prevSubsetStop += nextFoldSize\n",
    "        Nsubsets += 1\n",
    "    \n",
    "    subsetsList.append(ixSet[prevSubsetStop:])\n",
    "    Nsubsets += 1    \n",
    "    return (Nsubsets, subsetsList)\n",
    "\n",
    "#test\n",
    "#sub = foldsGenerator(E0_data_tr.index, 40, 55, 700)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Scores functions implementation\n",
    "\n",
    "def brierScore(probW, probL, probD, true, classLabels):\n",
    "    \n",
    "    trueW = true.map(lambda x : 1 if x == classLabels['W'] else 0).values\n",
    "    trueL = true.map(lambda x : 1 if x == classLabels['L'] else 0).values\n",
    "    trueD = true.map(lambda x : 1 if x == classLabels['D'] else 0).values\n",
    "    \n",
    "    cumulScore = (probW - trueW)*(probW - trueW) + (probL - trueL)*(probL - trueL) + (probD - trueD)*(probD - trueD)\n",
    "    \n",
    "    return float(np.sum(cumulScore))/float(true.index.size)\n",
    "\n",
    "def rankProbabilityScore(probW, probL, probD, true, classLabels):\n",
    "    trueW = true.map(lambda x : 1 if x == classLabels['W'] else 0).values\n",
    "    trueL = true.map(lambda x : 1 if x == classLabels['L'] else 0).values\n",
    "    trueD = true.map(lambda x : 1 if x == classLabels['D'] else 0).values\n",
    "    \n",
    "    true1 = trueL\n",
    "    true2 = trueL + trueD\n",
    "    \n",
    "    prob1 = probL\n",
    "    prob2 = probL + probD\n",
    "    \n",
    "    cumulScore = (prob1 - true1)*(prob1 - true1) + (prob2 - true2) * (prob2 - true2)\n",
    "    \n",
    "    return(float(np.sum(cumulScore))/(2.0 * float(true.index.size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Score estimation function \n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def crossValidation(predictors, labels, classLabels, foldGenParams, fitPredFun, fittingParams, scoreFunList, NObsMax = -1):\n",
    "    \n",
    "    (Nfold, subsetsList) = foldsGenerator(labels.index, foldGenParams['foldMinSize'], \n",
    "                                          foldGenParams['foldMaxSize'], foldGenParams['trInitSize'])\n",
    "    trSubset = []\n",
    "    validSubset = subsetsList.pop(0)\n",
    "    \n",
    "    k=0\n",
    "    avScores = np.zeros(3 + len(scoreFunList))\n",
    "    while (len(subsetsList) > 0) :\n",
    "        if (k == 0):                \n",
    "            trSubset = validSubset\n",
    "        else:\n",
    "            trSubset = trSubset.append(validSubset)\n",
    "        if (NObsMax != (-1)):\n",
    "            trSubset = trSubset[-NObsMax:]\n",
    "        \n",
    "        validSubset = subsetsList.pop(0)\n",
    "    \n",
    "        predictors_tr = predictors.loc[trSubset,:]\n",
    "        predictors_val = predictors.loc[validSubset,:]\n",
    "        labels_tr = labels.loc[trSubset]\n",
    "        labels_val = labels.loc[validSubset]\n",
    "\n",
    "        #model fitting + probabilities prediction\n",
    "        (predLabels, probW, probL, probD) = fitPredFun(predictors_tr, labels_tr, predictors_val, labels_val, fittingParams)\n",
    "\n",
    "        scores = []\n",
    "        scores.append(float(len(predictors_val.index)) * accuracy_score(predLabels, labels_val.values))\n",
    "        if len(labels_val.unique()) > 2:\n",
    "            scores.append(float(len(predictors_val.index)) * precision_score(predLabels, labels_val, average='weighted'))\n",
    "            scores.append(float(len(predictors_val.index)) * recall_score(predLabels, labels_val, average='weighted'))\n",
    "        else:\n",
    "            scores.append(float(len(predictors_val.index)) * precision_score(predLabels, labels_val, pos_label='W'))\n",
    "            scores.append(float(len(predictors_val.index)) * recall_score(predLabels, labels_val, pos_label='W'))\n",
    "        \n",
    "        for scoreFun in scoreFunList:\n",
    "            scores.append(float(len(predictors_val.index)) * scoreFun(probW, probL, probD, labels_val, classLabels))\n",
    "        print(scoreFun(probW, probL, probD, labels_val, classLabels))\n",
    "        avScores = avScores + np.array(scores)\n",
    "        \n",
    "        k += 1\n",
    "    return avScores/float(len(labels.index) - foldGenParams['trInitSize'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from IPython.display import display\n",
    "\n",
    "#Create X and Y\n",
    "X = pd.get_dummies(E0_data[['HomeTeam', 'AwayTeam', 'HTeamEloScore', 'ATeamEloScore', 'HTdaysSinceLastMatch',\n",
    "                            'ATdaysSinceLastMatch', 'HTdaysSinceLastMatch', 'ATdaysSinceLastMatch', 'HTW_rate', 'ATW_rate',\n",
    "                            'ATD_rate', 'HTD_rate', '1_last_HTR_isW', '1_last_HTR_isL', '1_last_ATR_isW', '1_last_ATR_isL']])\n",
    "Y = E0_data[['FTR']]\n",
    "\n",
    "#X preprocessing\n",
    "imputer = SimpleImputer()\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "#Split X and Y into training and Test Sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_imputed, Y, shuffle=True)\n",
    "\n",
    "#Logistic Regression Model Setup\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "#Logistic Regression Model Metrics\n",
    "print(\"Logestic Regression\")\n",
    "print(\"Train Score: \", model.score(x_train, y_train))\n",
    "print(\"Test Score: \", model.score(x_test, y_test))\n",
    "print(classification_report(y_test, model.predict(x_test), digits=3))\n",
    "\n",
    "#Forest model setup\n",
    "forest = RandomForestClassifier(n_estimators=2, random_state=2)\n",
    "forest.fit(x_train, y_train)\n",
    "\n",
    "#Forest Model Metrics\n",
    "print(\"Forest Classifier\")\n",
    "print(\"Train Score: \", forest.score(x_train, y_train))\n",
    "print(\"Test Score: \", forest.score(x_test, y_test))\n",
    "print(classification_report(y_test, forest.predict(x_test), digits=3))\n",
    "\n",
    "print(y_test.shape)\n",
    "print(forest.predict(x_test).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "display(E0_data.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "#make TF work like the TF in the HWs\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "#Other feature selections\n",
    "'''#X = pd.get_dummies(E0_data[['HomeTeam', 'AwayTeam', 'HTeamEloScore', 'ATeamEloScore', 'HTdaysSinceLastMatch',\n",
    "                            'ATdaysSinceLastMatch']])\n",
    "X = pd.get_dummies(E0_data[['HomeTeam', 'AwayTeam', 'FTAG', 'FTHG']])\n",
    "X = pd.get_dummies(E0_data[['HomeTeam', 'AwayTeam', 'FTR']])\n",
    "X = pd.get_dummies(E0_data.drop(['FTR',  'HTAG', 'Date', 'matchID', 'HW', 'AW', 'D', 'AR',\n",
    "                                 'ordinalHR', 'Season'], axis=1))'''\n",
    "\n",
    "#Setup Data\n",
    "X = pd.get_dummies(E0_data[['HomeTeam', 'AwayTeam', 'HTeamEloScore', 'ATeamEloScore', 'HTdaysSinceLastMatch',\n",
    "                            'ATdaysSinceLastMatch', 'HTdaysSinceLastMatch', 'ATdaysSinceLastMatch', 'HTW_rate', 'ATW_rate',\n",
    "                            'ATD_rate', 'HTD_rate', '1_last_HTR_isW', '1_last_HTR_isL', '1_last_ATR_isW', '1_last_ATR_isL']])\n",
    "Y = E0_data[['ordinalHR']].to_numpy().ravel()*2\n",
    "\n",
    "\n",
    "\n",
    "#X preprocessing\n",
    "imputer = SimpleImputer()\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "#Split X and Y into training and Test Sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_imputed, Y, shuffle=True)\n",
    "\n",
    "#Neural Network Setup\n",
    "n_inputs = X.shape[1]\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 3\n",
    "\n",
    "#Tensorflow X and Y\n",
    "X = tf.placeholder(tf.float32, shape = (None, n_inputs), name = 'X')\n",
    "Y = tf.placeholder(tf.int32, shape = (None), name = 'Y')\n",
    "\n",
    "#Function used to better display model metrics\n",
    "def convert_ordinalHR(x):\n",
    "    y=[]\n",
    "    for i in range(x.size):\n",
    "        if x[i] == 0:\n",
    "            y.append('A')\n",
    "        elif x[i] == 2:\n",
    "            y.append('H')\n",
    "        elif x[i] == 1:\n",
    "            y.append('D')\n",
    "    return y\n",
    "\n",
    "#General Function for neural layer setup\n",
    "def neuron_layer(X, n_neurons, name, activation = None):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        stddev = 2 / np.sqrt(n_inputs + n_neurons)\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)\n",
    "        W = tf.Variable(init, name = 'kernel')\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name='bias')\n",
    "        L2 = tf.nn.l2_loss(W)\n",
    "        Z = tf.matmul(X,W)+b\n",
    "        if activation is not None:\n",
    "            return activation(Z), L2\n",
    "        else:\n",
    "            return Z, L2\n",
    "\n",
    "#Tensorflow neural layer setup\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1, L2_1 = neuron_layer(X, n_hidden1, name='hidden1', activation=tf.nn.relu)\n",
    "    hidden2, L2_2 = neuron_layer(hidden1, n_hidden2, name='hidden2', activation=tf.nn.relu)\n",
    "    logits, L2_3 = neuron_layer(hidden2, n_outputs, name='outputs', activation=None)\n",
    "\n",
    "\n",
    "\n",
    "#Loss function\n",
    "beta=0.01\n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = Y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy+beta*(L2_1+L2_2+L2_3), name='loss')\n",
    "    \n",
    "\n",
    "\n",
    "#Optimizer\n",
    "learning_rate = 0.01\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "#Metric analysis setup\n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits, Y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    preds = tf.argmax(input=logits, axis=1)\n",
    "    \n",
    "#Initialize the above tensorflow variables    \n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#Runtime and batchsetup\n",
    "n_epochs = 10000\n",
    "batch_size = 50\n",
    "\n",
    "#Run the model\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    #Model Loop\n",
    "    for epoch in range(n_epochs):\n",
    "        sess.run(training_op, feed_dict={X: x_train, Y: y_train})\n",
    "        acc_train = accuracy.eval(feed_dict={X: x_train, Y: y_train})\n",
    "        acc_val = accuracy.eval(feed_dict={X: x_test, Y: y_test})\n",
    "        if(epoch % 200 == 0 or epoch==n_epochs-1):\n",
    "            print(epoch, 'Train accuracy:', acc_train, 'Val accuracy:', acc_val)\n",
    "    \n",
    "    #Retrive Metrics\n",
    "    acc_val = accuracy.eval(feed_dict={X: x_test, Y: y_test})\n",
    "    preds = preds.eval(feed_dict = {X:x_test})\n",
    "    log = logits.eval(feed_dict = {X:x_test})\n",
    "    preds=convert_ordinalHR(preds)\n",
    "    y_test=convert_ordinalHR(y_test)\n",
    "    \n",
    "    #Print Metrics\n",
    "    print('Train accuracy:', acc_train, 'Val accuracy:', acc_val)\n",
    "    print(classification_report(y_test, preds, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:52:10) \n[Clang 14.0.6 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "e35c5856cff72d20e6e0e19445c0d45b03888a7b19367444be448131dd51e693"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
