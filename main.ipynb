{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import needed Python libraries\n",
    "import numpy as np\n",
    "import csv\n",
    "import random\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as scistats\n",
    "import math\n",
    "import pylab\n",
    "import statsmodels as sm\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graphics parameters of the notebook\n",
    "# display graphs inline\n",
    "%matplotlib inline\n",
    "\n",
    "# Make graphs prettier\n",
    "pd.set_option('display.max_columns', 15)\n",
    "pd.set_option('display.width', 400)\n",
    "pd.set_option('plotting.matplotlib.register_converters', True)\n",
    "\n",
    "# Make the fonts bigger\n",
    "plt.rc('figure', figsize=(14, 7))\n",
    "plt.rc('font', family='normal', weight='bold', size=15)\n",
    "#inegrate data from 2009-2010 to 2018-2019 seasons from different files\n",
    "data_18_19 = pd.read_csv(\"./data/2018_2019.csv\", parse_dates=True)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
    "data_17_18 = pd.read_csv(\"./data/2017_2018.csv\", parse_dates=True)\n",
    "data_16_17 = pd.read_csv(\"./data/2016_2017.csv\", parse_dates=True)\n",
    "data_15_16 = pd.read_csv(\"./data/2015_2016.csv\", parse_dates=True)\n",
    "data_14_15 = pd.read_csv(\"./data/2014_2015.csv\", parse_dates=True)\n",
    "data_13_14 = pd.read_csv(\"./data/2013_2014.csv\", parse_dates=True)\n",
    "data_12_13 = pd.read_csv(\"./data/2012_2013.csv\", parse_dates=True)\n",
    "data_11_12 = pd.read_csv(\"./data/2011_2012.csv\", parse_dates=True)\n",
    "data_10_11 = pd.read_csv(\"./data/2010_2011.csv\", parse_dates=True)\n",
    "data_09_10 = pd.read_csv(\"./data/2009_2010.csv\", parse_dates=True)\n",
    "data_08_09 = pd.read_csv(\"./data/2008_2009.csv\", parse_dates=True)\n",
    "#data_07_08 = pd.read_csv(\"./data/2007_2008.csv\", parse_dates=True)\n",
    "#data_06_07 = pd.read_csv(\"./data/2006_2007.csv\", parse_dates=True)\n",
    "#data_05_06 = pd.read_csv(\"./data/2005_2006.csv\", parse_dates=True)\n",
    "\n",
    "#integrate data in a single df\n",
    "raw_data = pd.concat([data_18_19, data_17_18, data_16_17, data_15_16, data_14_15, data_13_14, data_12_13, data_11_12, data_10_11, data_09_10, data_08_09])\n",
    "print(raw_data)\n",
    "\n",
    "#Select useful features for datavisualization and analysis purposes\n",
    "data_table = raw_data[[\"Date\", \"HomeTeam\", \"AwayTeam\", \"FTHG\", \"FTAG\",\n",
    "                    \"FTR\", \"HTAG\", 'B365A', 'B365D', 'B365H', 'BSA', \n",
    "                    'BSD', 'BSH', 'BWA', 'BWD', 'BWH', 'GBA', 'GBD',\n",
    "                    'GBH', 'IWA', 'IWD', 'IWH', 'LBA', 'LBD', 'LBH',\n",
    "                    'PSA', 'PSD', 'PSH', 'SBA', 'SBD', 'SBH', 'SJA',\n",
    "                    'SJD', 'SJH', 'VCA', 'VCD', 'VCH', 'WHA','WHD', 'WHH']]\n",
    "\n",
    "#convert date format to YYYY-MM-DD classic format\n",
    "data_table.Date = data_table.Date.map(lambda x : \"20\" + x[6:8] + \"-\" + x[3:5] + \"-\" + x[0:2])\n",
    "\n",
    "#sort data by date\n",
    "data_table.sort_values('Date', inplace=True)\n",
    "\n",
    "#reset data indexes\n",
    "data_table = data_table.reset_index(drop=True)\n",
    "\n",
    "#create matchID column\n",
    "data_table['matchID'] = data_table.index\n",
    "\n",
    "#create season feature\n",
    "data_table['Season'] = 0\n",
    "data_table.Season = data_table.Date.map(lambda x : int(x[0:4]) if int(x[5:7]) > 6 else int(x[0:4]) - 1)\n",
    "\n",
    "#null values test\n",
    "data_table.isnull().any()\n",
    "\n",
    "#create teams list\n",
    "teams = data_table['HomeTeam'].unique()\n",
    "print(teams)\n",
    "\n",
    "#create seasons list\n",
    "seasons = np.sort(data_table['Season'].unique())\n",
    "print(seasons)\n",
    "\n",
    "#create dictionary containing teams list by season\n",
    "teams_by_season = {season : data_table[data_table['Season'] == season]['HomeTeam'].unique() for season in seasons}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#match day feature construction for HomeTeam and AwayTeam (1st match of a season --> 1, last --> 38 because 20 team play by season)\n",
    "data_table_HT_grpby = data_table.groupby('HomeTeam')[['Date']]\n",
    "data_table_AT_grpby = data_table.groupby('AwayTeam')[['Date']]\n",
    "\n",
    "# Calcola la giornata di campionato\n",
    "def row2matchWeek_HT(row):\n",
    "    x = row['HomeTeam']\n",
    "    y = row['Date']\n",
    "    df1 = data_table_HT_grpby.get_group(x)\n",
    "    df2 = data_table_AT_grpby.get_group(x)\n",
    "    df1 = df1[df1['Date'] < y]\n",
    "    df2 = df2[df2['Date'] < y]\n",
    "    day = (1 + len(df1) + len(df2)) % 38\n",
    "    return 38 if day == 0 else day \n",
    "\n",
    "def row2matchWeek_AT(row):\n",
    "    x = row['AwayTeam']\n",
    "    y = row['Date']\n",
    "    df1 = data_table_HT_grpby.get_group(x)\n",
    "    df2 = data_table_AT_grpby.get_group(x)\n",
    "    df1 = df1[df1['Date'] < y]\n",
    "    df2 = df2[df2['Date'] < y]\n",
    "    day = (1 + len(df1) + len(df2)) % 38\n",
    "    return 38 if day == 0 else day\n",
    "\n",
    "data_table['HomeTeamDay'] = data_table.apply(row2matchWeek_HT, axis=1)\n",
    "data_table['AwayTeamDay'] = data_table.apply(row2matchWeek_AT, axis=1)\n",
    "\n",
    "# Calcolo il numero di partite giocate in casa e fuori per ogni team in ogni stagione\n",
    "data_table['ones'] = 1\n",
    "for season in seasons:\n",
    "    for team in teams_by_season[season]:\n",
    "        sH = data_table[(data_table['HomeTeam'] == team) & (data_table['Season'] == season)]['ones']\n",
    "        data_table.loc[sH.index, 'HomeTeamHomeDay'] = sH.cumsum()\n",
    "        \n",
    "        sA = data_table[(data_table['AwayTeam'] == team) & (data_table['Season'] == season)]['ones']\n",
    "        data_table.loc[sA.index, 'AwayTeamAwayDay'] = sA.cumsum()\n",
    "        \n",
    "# Nel dataset ho una colonna FTR (full time result) che riporta H se ha vinto la squadra in casa, altrimenti A\n",
    "def resultConverter(A):\n",
    "    if A == 'H':\n",
    "        return 'W'\n",
    "    elif A =='A':\n",
    "        return 'L'\n",
    "    else:\n",
    "        return 'D'\n",
    "\n",
    "def resultInverser(A):\n",
    "    if A == 'W':\n",
    "        return 'L'\n",
    "    elif A == 'L':\n",
    "        return 'W'\n",
    "    else:\n",
    "        return 'D'\n",
    "        \n",
    "def ordinalResultConverter(A):\n",
    "    if A == 'W':\n",
    "        return 1\n",
    "    elif A == 'L':\n",
    "        return 0\n",
    "    else:\n",
    "        return 0.5\n",
    "\n",
    "#make dummies variables for FTR (result of match), HW = Home Win, AW = Away Win, D = draw\n",
    "data_table['HW'] = data_table.FTR.map(lambda x : 1 if x == 'H' else 0)\n",
    "data_table['AW'] = data_table.FTR.map(lambda x : 1 if x == 'A' else 0)\n",
    "data_table['D']= data_table.FTR.map(lambda x : 1 if x == 'D' else 0)\n",
    "\n",
    "#make 2 different variable for the result of a match : 1 for the home team point of view, the other for the away team pt of view\n",
    "data_table['HR'] = data_table.FTR.map(lambda x : resultConverter(x))\n",
    "data_table['AR'] = data_table.HR.map(lambda x : resultInverser(x))\n",
    "\n",
    "#make ordinal variable for the home team point of view result (1 = win, 0.5 = Draw, 0 = loss)\n",
    "data_table['ordinalHR'] = data_table.HR.map(lambda x : ordinalResultConverter(x))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_by_HT = data_table.groupby('HomeTeam')\n",
    "grp_by_AT = data_table.groupby('AwayTeam')\n",
    " \n",
    "grp_by_HT_and_season = data_table.groupby(['HomeTeam', 'Season'])\n",
    "grp_by_AT_and_season = data_table.groupby(['AwayTeam', 'Season'])\n",
    "\n",
    "#past performance features engineering\n",
    "for team in teams:\n",
    "    \n",
    "    #we retrieve results series of the team\n",
    "    teamHomeResults_s = grp_by_HT.get_group(team)['HR']\n",
    "    teamAwayResults_s = grp_by_AT.get_group(team)['AR']\n",
    "    #combine these 2 series and sort the obtained serie\n",
    "    teamResults_s = pd.concat([teamHomeResults_s, teamAwayResults_s]).sort_index()\n",
    "\n",
    "    #(i) compute k_last_HR and k_last_AR --> 6 features\n",
    "    # Dizionario {<partita>:<risultato>}\n",
    "    # {0: nan, 21: 'W', 49: 'W', 69: 'L', 96: 'W', 113: 'D', ..., 4073: 'W', 4097: 'W', 4113: 'W', 4142: 'W', 4166: 'L'}\n",
    "    # TODO: commenta cosa fa\n",
    "    lag1TeamResults_d = teamResults_s.shift(1).to_dict()\n",
    "    lag2TeamResults_d = teamResults_s.shift(2).to_dict()\n",
    "    lag3TeamResults_d = teamResults_s.shift(3).to_dict()\n",
    "    \n",
    "    #k_last_HTR and k_last_ATR are just shifted versions of the results series\n",
    "    data_table.loc[teamHomeResults_s.index,'1_last_HTR'] = data_table.loc[teamHomeResults_s.index,:].index.map(lambda x : lag1TeamResults_d[x])\n",
    "    data_table.loc[teamHomeResults_s.index,'2_last_HTR'] = data_table.loc[teamHomeResults_s.index,:].index.map(lambda x : lag2TeamResults_d[x])\n",
    "    data_table.loc[teamHomeResults_s.index,'3_last_HTR'] = data_table.loc[teamHomeResults_s.index,:].index.map(lambda x : lag3TeamResults_d[x])\n",
    "    data_table.loc[teamAwayResults_s.index,'1_last_ATR'] = data_table.loc[teamAwayResults_s.index,:].index.map(lambda x : lag1TeamResults_d[x])\n",
    "    data_table.loc[teamAwayResults_s.index,'2_last_ATR'] = data_table.loc[teamAwayResults_s.index,:].index.map(lambda x : lag2TeamResults_d[x])\n",
    "    data_table.loc[teamAwayResults_s.index,'3_last_ATR'] = data_table.loc[teamAwayResults_s.index,:].index.map(lambda x : lag3TeamResults_d[x])\n",
    "    \n",
    "    #(ii) Compute k_last_HTRH and k_last ATAR --> 4 features\n",
    "    #we need here to diferentiate home results and past results. Python dictionaries allows the program to access to\n",
    "    #needed data faster than with a Pandas serie\n",
    "    # Dovendo predire il risultato finale, assumiamo che sia più dipendente dai risultati a fine match (full time - FT), \n",
    "    # piuttosto che dai risultati a metà match (half time - HT) => consideriamo quindi sono gli half time dell'ultima e penultima partita\n",
    "    lag1TeamHomeResults_d = teamHomeResults_s.shift(1).to_dict()\n",
    "    lag2TeamHomeResults_d = teamHomeResults_s.shift(2).to_dict()\n",
    "    lag1TeamAwayResults_d = teamAwayResults_s.shift(1).to_dict()\n",
    "    lag2TeamAwayResults_d = teamAwayResults_s.shift(2).to_dict()\n",
    "    \n",
    "    data_table.loc[teamHomeResults_s.index,'1_last_HTHR'] = data_table.loc[teamHomeResults_s.index,:].index.map(lambda x : lag1TeamHomeResults_d[x])\n",
    "    data_table.loc[teamHomeResults_s.index,'2_last_HTHR'] = data_table.loc[teamHomeResults_s.index,:].index.map(lambda x : lag2TeamHomeResults_d[x])\n",
    "    data_table.loc[teamAwayResults_s.index,'1_last_ATAR'] = data_table.loc[teamAwayResults_s.index,:].index.map(lambda x : lag1TeamAwayResults_d[x])\n",
    "    data_table.loc[teamAwayResults_s.index,'2_last_ATAR'] = data_table.loc[teamAwayResults_s.index,:].index.map(lambda x : lag2TeamAwayResults_d[x])\n",
    "    \n",
    "    #(iii) rates based features : we need to get only season specific results series (to avoid taking previous season results into season rates)\n",
    "    for season in seasons:\n",
    "        \n",
    "        if team in teams_by_season[season]:\n",
    "            #retrieve season specific results serie (1 win serie, 1 draw serie the loss  will be computed thanks to\n",
    "            #the 2 others)\n",
    "            teamHomeResultsW_s = grp_by_HT_and_season.get_group((team,season))['HW']\n",
    "            teamAwayResultsW_s = grp_by_AT_and_season.get_group((team,season))['AW']\n",
    "            teamResultsW_s = pd.concat([teamHomeResultsW_s, teamAwayResultsW_s]).sort_index()\n",
    "\n",
    "            teamHomeResultsD_s = grp_by_HT_and_season.get_group((team,season))['D']\n",
    "            teamAwayResultsD_s = grp_by_AT_and_season.get_group((team,season))['D']\n",
    "            teamResultsD_s = pd.concat([teamHomeResultsD_s, teamAwayResultsD_s]).sort_index()\n",
    "        \n",
    "            #(0) compute HW rates, HL rates, AW rates, LW rates since begining of season\n",
    "            teamResultsWCumul_d = teamResultsW_s.shift(1).cumsum().to_dict()\n",
    "            teamResultsDCumul_d = teamResultsD_s.shift(1).cumsum().to_dict()\n",
    "\n",
    "            #(i) compute 7_HTW_rate, 12_HTW_rate, 7_HTD_rate, 12_HTD_rate, 7_ATW_rate, 12_ATW_rate, 7_ATD_rate, 12_ATD_rate --> 8 features\n",
    "            # Esempio del 7_HTW_rate:\n",
    "            #   1. Mi posiziono su una partita X e voglio calcolare la media M dei risultati precedenti alla partita (X-1, X-2, ..., X-7), quindi shifto a dx\n",
    "            #       per non considerare nella M la partita stessa X e per avere M nella stessa posizione di X (sfrutto le stesse matrici su cui eseguo i calcoli) \n",
    "            #   2. Poi si fissa una finestra larga 7, e si calcola la media di un sottoinsieme di dati, ma per farlo è necessario \n",
    "            #       ci siano almeno 5 valori nella finestra. La finestra parte con il sottoinsieme di dati {0}, poi {0, 15} -> {0, 15, 21} -> ...\n",
    "            # {<matchID>: <media>}\n",
    "            # {0: nan, 15: nan, 21: nan, 30: nan, 43: nan, 49: 0.8, 60: 0.6666666666666666, 69: 0.5714285714285714, ...}\n",
    "            win7TeamResultsW_d = teamResultsW_s.shift(1).rolling(window = 7, min_periods = 5).mean().to_dict()\n",
    "            win12TeamResultsW_d = teamResultsW_s.shift(1).rolling(window = 12, min_periods = 8).mean().to_dict()\n",
    "            win7TeamResultsD_d = teamResultsD_s.shift(1).rolling( window = 7, min_periods = 5).mean().to_dict()\n",
    "            win12TeamResultsD_d = teamResultsD_s.shift(1).rolling( window = 12, min_periods = 8).mean().to_dict()\n",
    "        \n",
    "            data_table.loc[teamHomeResultsW_s.index,'HTW_rate'] = data_table.loc[teamHomeResultsW_s.index,:].index.map(lambda x : teamResultsWCumul_d[x])\n",
    "            data_table.loc[teamAwayResultsW_s.index,'ATW_rate'] = data_table.loc[teamAwayResultsW_s.index,:].index.map(lambda x : teamResultsWCumul_d[x])\n",
    "            data_table.loc[teamHomeResultsW_s.index,'HTD_rate'] = data_table.loc[teamHomeResultsW_s.index,:].index.map(lambda x : teamResultsDCumul_d[x])\n",
    "            data_table.loc[teamAwayResultsW_s.index,'ATD_rate'] = data_table.loc[teamAwayResultsW_s.index,:].index.map(lambda x : teamResultsDCumul_d[x])\n",
    "        \n",
    "            data_table.loc[teamHomeResultsW_s.index,'7_HTW_rate'] = data_table.loc[teamHomeResultsW_s.index,:].index.map(lambda x : win7TeamResultsW_d[x])\n",
    "            data_table.loc[teamHomeResultsW_s.index,'12_HTW_rate'] = data_table.loc[teamHomeResultsW_s.index,:].index.map(lambda x : win12TeamResultsW_d[x])\n",
    "            data_table.loc[teamAwayResultsW_s.index,'7_ATW_rate'] = data_table.loc[teamAwayResultsW_s.index,:].index.map(lambda x : win7TeamResultsW_d[x])\n",
    "            data_table.loc[teamAwayResultsW_s.index,'12_ATW_rate'] = data_table.loc[teamAwayResultsW_s.index,:].index.map(lambda x : win12TeamResultsW_d[x])\n",
    "        \n",
    "            data_table.loc[teamHomeResultsD_s.index,'7_HTD_rate'] = data_table.loc[teamHomeResultsD_s.index,:].index.map(lambda x : win7TeamResultsD_d[x])\n",
    "            data_table.loc[teamHomeResultsD_s.index,'12_HTD_rate'] = data_table.loc[teamHomeResultsD_s.index,:].index.map(lambda x : win12TeamResultsD_d[x])\n",
    "            data_table.loc[teamAwayResultsD_s.index,'7_ATD_rate'] = data_table.loc[teamAwayResultsD_s.index,:].index.map(lambda x : win7TeamResultsD_d[x])\n",
    "            data_table.loc[teamAwayResultsD_s.index,'12_ATD_rate'] = data_table.loc[teamAwayResultsD_s.index,:].index.map(lambda x : win12TeamResultsD_d[x])\n",
    "\n",
    "        #(ii) compute 5_HTHW_rate and 5_ATAW_rate\n",
    "        win5TeamResultsHomeW_d = teamHomeResultsW_s.shift(1).rolling( window = 5, min_periods = 3).mean().to_dict()\n",
    "        win5TeamResultsAwayW_d = teamAwayResultsW_s.shift(1).rolling( window = 5, min_periods = 3).mean().to_dict()\n",
    "        data_table.loc[teamHomeResultsW_s.index,'5_HTHW_rate'] = data_table.loc[teamHomeResultsW_s.index,:].index.map(lambda x : win5TeamResultsHomeW_d[x])\n",
    "        data_table.loc[teamAwayResultsW_s.index,'5_ATAW_rate'] = data_table.loc[teamAwayResultsW_s.index,:].index.map(lambda x : win5TeamResultsAwayW_d[x])\n",
    "        \n",
    "        #(iii) compute HTHW_rate, ATAW_rate, HTHD_rate, ATAD_rate\n",
    "        teamHomeResultsCumulW_d = teamHomeResultsW_s.shift(1).cumsum().to_dict()\n",
    "        teamHomeResultsCumulD_d = teamHomeResultsD_s.shift(1).cumsum().to_dict()\n",
    "        teamAwayResultsCumulW_d = teamAwayResultsW_s.shift(1).cumsum().to_dict()\n",
    "        teamAwayResultsCumulD_d = teamAwayResultsD_s.shift(1).cumsum().to_dict()\n",
    "        data_table.loc[teamHomeResultsW_s.index,'HTHW_rate'] = data_table.loc[teamHomeResultsW_s.index,:].index.map(lambda x : teamHomeResultsCumulW_d[x])\n",
    "        data_table.loc[teamHomeResultsW_s.index,'HTHD_rate'] = data_table.loc[teamHomeResultsW_s.index,:].index.map(lambda x : teamHomeResultsCumulD_d[x])\n",
    "        data_table.loc[teamAwayResultsW_s.index,'ATAW_rate'] = data_table.loc[teamAwayResultsW_s.index,:].index.map(lambda x : teamAwayResultsCumulW_d[x])\n",
    "        data_table.loc[teamAwayResultsW_s.index,'ATAD_rate'] = data_table.loc[teamAwayResultsW_s.index,:].index.map(lambda x : teamAwayResultsCumulD_d[x])\n",
    "\n",
    "\n",
    "        \n",
    "#compute missing features k_XTL_rate thanks to the k_XTW_rate and k_XTD_rate features\n",
    "data_table.loc[:,'7_HTL_rate'] = 1 - (data_table['7_HTW_rate'] + data_table['7_HTD_rate'])\n",
    "data_table.loc[:,'12_HTL_rate'] = 1 - (data_table['7_HTW_rate'] + data_table['7_HTD_rate'])\n",
    "data_table.loc[:,'7_ATL_rate'] = 1 - (data_table['7_ATW_rate'] + data_table['7_ATD_rate'])\n",
    "data_table.loc[:,'12_ATL_rate'] = 1 - (data_table['7_ATW_rate'] + data_table['7_ATD_rate'])\n",
    "\n",
    "#compute missing HTL_rate, ATL_rate with features with the wins and draws features\n",
    "data_table.loc[:,'HTW_rate'] = data_table['HTW_rate']/data_table['HomeTeamDay']\n",
    "data_table.loc[:,'ATW_rate'] = data_table['ATW_rate']/data_table['AwayTeamDay']\n",
    "data_table.loc[:,'HTD_rate'] = data_table['HTD_rate']/data_table['HomeTeamDay']\n",
    "data_table.loc[:,'ATD_rate'] = data_table['ATD_rate']/data_table['AwayTeamDay']\n",
    "data_table.loc[:,'HTL_rate'] = 1 - (data_table['HTW_rate'] + data_table['HTD_rate'])\n",
    "data_table.loc[:,'ATL_rate'] = 1 - (data_table['ATW_rate'] + data_table['ATD_rate'])\n",
    "\n",
    "#we finish to compute HTHW_rate, ..., ATAD_rate features and compute corresponding loss features\n",
    "data_table.loc[:,'HTHW_rate'] = data_table['HTHW_rate']/data_table['HomeTeamHomeDay']\n",
    "data_table.loc[:,'ATAW_rate'] = data_table['ATAW_rate']/data_table['AwayTeamAwayDay']\n",
    "data_table.loc[:,'HTHD_rate'] = data_table['HTHD_rate']/data_table['HomeTeamHomeDay']\n",
    "data_table.loc[:,'ATAD_rate'] = data_table['ATAD_rate']/data_table['AwayTeamAwayDay']\n",
    "data_table.loc[:,'HTHL_rate'] = 1 - (data_table['HTHW_rate'] + data_table['HTHD_rate'])\n",
    "data_table.loc[:,'ATAL_rate'] = 1 - (data_table['ATAW_rate'] + data_table['ATAD_rate'])\n",
    "\n",
    "data_table.to_csv(\"./data/features.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELO score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elo ranking method parameters\n",
    "k = 20.0\n",
    "d = 400.0\n",
    "c = 10.0\n",
    "\n",
    "#Initialization of output containers\n",
    "ELO_dict = dict()\n",
    "gammaHT_dict = dict()\n",
    "gammaAT_dict = dict()\n",
    "\n",
    "#intermediate data containers initilization\n",
    "latest_update_date = dict() #contains latest updates in date of ELO_dict\n",
    "prev_date_ELO_score = dict() #contains latest ELO_score given to a team for computing new one\n",
    "\n",
    "prev_season_teams = [team for team in teams] #contains list of teams for the current season\n",
    "last_teams_ELO_average = 0.0 #contains ELO average of last previous season teams\n",
    "\n",
    "for team in teams:\n",
    "    latest_update_date[team] = '2001-01-01'\n",
    "    prev_date_ELO_score[team] = 0.0\n",
    "\n",
    "for season in data_table.Season.unique():\n",
    "    season_match_dates = data_table[data_table['Season'] == season].Date.unique()\n",
    "    season_teams = data_table[data_table['Season'] == season].HomeTeam.unique()\n",
    "    last_season_date = season_match_dates[len(season_match_dates) - 1]\n",
    "    \n",
    "    for Steam in season_teams:\n",
    "        if not (Steam in prev_season_teams):\n",
    "            prev_date_ELO_score[Steam] = last_teams_ELO_average\n",
    "            \n",
    "    for date in season_match_dates:\n",
    "        for team in teams:\n",
    "            # Se non trovo, per una certa data un certo team, allora per quel team e quella data l'ELO non cambia\n",
    "            if not ((team in data_table[data_table['Date'] == date]['HomeTeam'].values) | (team in data_table[data_table['Date'] == date]['AwayTeam'].values)):\n",
    "                ELO_dict[(team, date)] = prev_date_ELO_score[team]\n",
    "                latest_update_date[team] = date\n",
    "            # Altrimenti lo aggiorno sulla base del risultato\n",
    "            else:\n",
    "                if latest_update_date[team] < date:\n",
    "                    Hteam = data_table[(data_table['Date'] == date) & ((data_table['HomeTeam'] == team) | (data_table['AwayTeam'] == team))]['HomeTeam'].values[0]\n",
    "                    Ateam = data_table[(data_table['Date'] == date) & ((data_table['HomeTeam'] == team) | (data_table['AwayTeam'] == team))]['AwayTeam'].values[0]\n",
    "            \n",
    "                    l0H = prev_date_ELO_score[Hteam]\n",
    "                    l0A = prev_date_ELO_score[Ateam]\n",
    "                    gammaH = 1.0/(1.0 + c**((l0A - l0H)/d))\n",
    "                    gammaA = 1.0 - gammaH\n",
    "                    # .values ritorna un array quindi devo prendere il valore e non l'indice\n",
    "                    alphaH = data_table[(data_table['Date'] == date) & (data_table['HomeTeam'] == Hteam)]['ordinalHR'].values[0]\n",
    "                    alphaA = 1 - alphaH\n",
    "                    \n",
    "                    #compute new scores\n",
    "                    new_HT_ELO_score = l0H + k * (alphaH - gammaH)\n",
    "                    new_AT_ELO_score = l0A + k * (alphaA - gammaA)\n",
    "\n",
    "                    #put new scores in ELO_dict\n",
    "                    ELO_dict[(Hteam, date)] = new_HT_ELO_score\n",
    "                    ELO_dict[(Ateam, date)] = new_AT_ELO_score\n",
    "                    gammaHT_dict[(Hteam, date)] = gammaH\n",
    "                    gammaAT_dict[(Ateam, date)] = gammaA\n",
    "                    latest_update_date[Hteam] = date\n",
    "                    latest_update_date[Ateam] = date\n",
    "            \n",
    "                    #update prev_date_ELO_score and latest_update_date\n",
    "                    prev_date_ELO_score[Hteam] = new_HT_ELO_score\n",
    "                    prev_date_ELO_score[Ateam] = new_AT_ELO_score\n",
    "        \n",
    "        if date == last_season_date:\n",
    "            ELOs = np.array([prev_date_ELO_score[Steam] for Steam in season_teams])\n",
    "            ELOs.sort()\n",
    "            last_teams_ELO_average = np.mean(ELOs[0:-17])\n",
    "            prev_season_teams = season_teams\n",
    "            \n",
    "                        \n",
    "#make HTeamEloScore, ATeamEloScore and gammaHome features from previously computed dictionaries\n",
    "\n",
    "def HomeTeamEloScore(row):\n",
    "    return ELO_dict[(row['HomeTeam'], row['Date'])]\n",
    "\n",
    "def AwayTeamEloScore(row):\n",
    "    return ELO_dict[(row['AwayTeam'], row['Date'])]\n",
    "\n",
    "def gammaHTeamDate(row):\n",
    "    return gammaHT_dict[(row['HomeTeam'], row['Date'])]\n",
    "\n",
    "#compute resulting Elo scores important features\n",
    "data_table.loc[:,'HTeamEloScore'] = data_table.apply(HomeTeamEloScore, axis=1) \n",
    "data_table.loc[:,'ATeamEloScore'] = data_table.apply(AwayTeamEloScore, axis=1) \n",
    "data_table.loc[:,'gammaHome'] = data_table.apply(gammaHTeamDate, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%% Team tiredness feature extraction\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for team in teams:\n",
    "    homeMatchDates_s = data_table[data_table['HomeTeam'] == team]['Date']\n",
    "    awayMatchDates_s = data_table[data_table['AwayTeam'] == team]['Date']\n",
    "    matchDates_s = pd.concat([homeMatchDates_s, awayMatchDates_s]).sort_index()\n",
    "    lastMatchDates_s = matchDates_s.shift(1)\n",
    "    matchDates = matchDates_s.values\n",
    "        \n",
    "    data_table.loc[data_table['HomeTeam'] == team, 'HTLastMatchDate'] = data_table.loc[data_table['HomeTeam'] == team].index.map(lambda x : lastMatchDates_s[x])\n",
    "    data_table.loc[data_table['AwayTeam'] == team, 'ATLastMatchDate'] = data_table.loc[data_table['AwayTeam'] == team].index.map(lambda x : lastMatchDates_s[x])\n",
    "    \n",
    "def HTdaysBetweenDates(row):\n",
    "    if not (pd.isnull(row['HTLastMatchDate'])):\n",
    "        currDate = pd.to_datetime(row['Date'])\n",
    "        prevDate = pd.to_datetime(row['HTLastMatchDate'])\n",
    "        ndays = (currDate - prevDate).days \n",
    "        if ndays < 20:\n",
    "            return ndays\n",
    "        else: \n",
    "            return np.nan\n",
    "    else:\n",
    "        return np.nan \n",
    "    \n",
    "def ATdaysBetweenDates(row):\n",
    "    if not (pd.isnull(row['ATLastMatchDate'])):\n",
    "        currDate = pd.to_datetime(row['Date'])\n",
    "        prevDate = pd.to_datetime(row['ATLastMatchDate'])\n",
    "        return (currDate - prevDate).days\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "data_table.loc[:, 'HTdaysSinceLastMatch'] = data_table.apply(HTdaysBetweenDates, axis=1)\n",
    "data_table.loc[:, 'ATdaysSinceLastMatch'] = data_table.apply(ATdaysBetweenDates, axis=1)\n",
    "data_table.loc[:,'DaysSinceLastMatchRate'] = data_table['HTdaysSinceLastMatch'].astype(float)/data_table['ATdaysSinceLastMatch'].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "data_table['1_last_HTR_isW'] = data_table['1_last_HTR'].map(lambda x : 1 if x == 'W' else 0)\n",
    "data_table['1_last_HTR_isL'] = data_table['1_last_HTR'].map(lambda x : 1 if x == 'L' else 0) \n",
    "data_table['2_last_HTR_isW'] = data_table['2_last_HTR'].map(lambda x : 1 if x == 'W' else 0)\n",
    "data_table['2_last_HTR_isL'] = data_table['2_last_HTR'].map(lambda x : 1 if x == 'L' else 0) \n",
    "data_table['3_last_HTR_isW'] = data_table['3_last_HTR'].map(lambda x : 1 if x == 'W' else 0)\n",
    "data_table['3_last_HTR_isL'] = data_table['3_last_HTR'].map(lambda x : 1 if x == 'L' else 0) \n",
    "\n",
    "data_table['1_last_ATR_isW'] = data_table['1_last_ATR'].map(lambda x : 1 if x == 'W' else 0)\n",
    "data_table['1_last_ATR_isL'] = data_table['1_last_ATR'].map(lambda x : 1 if x == 'L' else 0) \n",
    "data_table['2_last_ATR_isW'] = data_table['2_last_ATR'].map(lambda x : 1 if x == 'W' else 0)\n",
    "data_table['2_last_ATR_isL'] = data_table['2_last_ATR'].map(lambda x : 1 if x == 'L' else 0) \n",
    "data_table['3_last_ATR_isW'] = data_table['3_last_ATR'].map(lambda x : 1 if x == 'W' else 0)\n",
    "data_table['3_last_ATR_isL'] = data_table['3_last_ATR'].map(lambda x : 1 if x == 'L' else 0) \n",
    "\n",
    "data_table['1_last_HTHR_isW'] = data_table['1_last_HTHR'].map(lambda x : 1 if x == 'W' else 0)\n",
    "data_table['1_last_HTHR_isL'] = data_table['1_last_HTHR'].map(lambda x : 1 if x == 'L' else 0) \n",
    "data_table['2_last_HTHR_isW'] = data_table['2_last_HTHR'].map(lambda x : 1 if x == 'W' else 0)\n",
    "data_table['2_last_HTHR_isL'] = data_table['2_last_HTHR'].map(lambda x : 1 if x == 'L' else 0)\n",
    "\n",
    "data_table['1_last_ATAR_isW'] = data_table['1_last_ATAR'].map(lambda x : 1 if x == 'W' else 0)\n",
    "data_table['1_last_ATAR_isL'] = data_table['1_last_ATAR'].map(lambda x : 1 if x == 'L' else 0) \n",
    "data_table['2_last_ATAR_isW'] = data_table['2_last_ATAR'].map(lambda x : 1 if x == 'W' else 0)\n",
    "data_table['2_last_ATAR_isL'] = data_table['2_last_ATAR'].map(lambda x : 1 if x == 'L' else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Folds generations custom function \n",
    "\n",
    "\n",
    "def foldsGenerator(ixSet, foldMinSize, foldMaxSize, trInitSize, trOptimalSize = -1):\n",
    "    \n",
    "    subsetsList = []\n",
    "    subsetsList.append(ixSet[0:trInitSize])\n",
    "    Nsubsets = 1\n",
    "    \n",
    "    ixSetLength = ixSet.size\n",
    "    \n",
    "    unfoldedSetSize = ixSetLength - trInitSize\n",
    "    prevSubsetStop = trInitSize\n",
    "    \n",
    "    while (unfoldedSetSize > foldMaxSize):\n",
    "        nextFoldSize = random.randint(foldMinSize, foldMaxSize)\n",
    "        \n",
    "        subsetsList.append(ixSet[prevSubsetStop:(prevSubsetStop + nextFoldSize)])\n",
    "        \n",
    "        unfoldedSetSize -= nextFoldSize\n",
    "        prevSubsetStop += nextFoldSize\n",
    "        Nsubsets += 1\n",
    "    \n",
    "    subsetsList.append(ixSet[prevSubsetStop:])\n",
    "    Nsubsets += 1    \n",
    "    return (Nsubsets, subsetsList)\n",
    "\n",
    "#test\n",
    "#sub = foldsGenerator(data_table_tr.index, 40, 55, 700)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Scores functions implementation\n",
    "\n",
    "def brierScore(probW, probL, probD, true, classLabels):\n",
    "    \n",
    "    trueW = true.map(lambda x : 1 if x == classLabels['W'] else 0).values\n",
    "    trueL = true.map(lambda x : 1 if x == classLabels['L'] else 0).values\n",
    "    trueD = true.map(lambda x : 1 if x == classLabels['D'] else 0).values\n",
    "    \n",
    "    cumulScore = (probW - trueW)*(probW - trueW) + (probL - trueL)*(probL - trueL) + (probD - trueD)*(probD - trueD)\n",
    "    \n",
    "    return float(np.sum(cumulScore))/float(true.index.size)\n",
    "\n",
    "def rankProbabilityScore(probW, probL, probD, true, classLabels):\n",
    "    trueW = true.map(lambda x : 1 if x == classLabels['W'] else 0).values\n",
    "    trueL = true.map(lambda x : 1 if x == classLabels['L'] else 0).values\n",
    "    trueD = true.map(lambda x : 1 if x == classLabels['D'] else 0).values\n",
    "    \n",
    "    true1 = trueL\n",
    "    true2 = trueL + trueD\n",
    "    \n",
    "    prob1 = probL\n",
    "    prob2 = probL + probD\n",
    "    \n",
    "    cumulScore = (prob1 - true1)*(prob1 - true1) + (prob2 - true2) * (prob2 - true2)\n",
    "    \n",
    "    return(float(np.sum(cumulScore))/(2.0 * float(true.index.size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from IPython.display import display\n",
    "\n",
    "#Create X and Y\n",
    "X = pd.get_dummies(data_table[['HomeTeam', 'AwayTeam', 'HTeamEloScore', 'ATeamEloScore', 'HTdaysSinceLastMatch',\n",
    "                            'ATdaysSinceLastMatch', 'HTW_rate', 'ATW_rate',\n",
    "                            'ATD_rate', 'HTD_rate', \n",
    "                '7_HTW_rate', '12_HTW_rate', '7_ATW_rate', '12_ATW_rate', \n",
    "                '7_HTD_rate', '12_HTD_rate', '7_ATD_rate', '12_ATD_rate',\n",
    "                '7_HTL_rate', '12_HTL_rate', '7_ATL_rate', '12_ATL_rate',\n",
    "                '5_HTHW_rate', '5_ATAW_rate']])\n",
    "Y = data_table[['FTR']]\n",
    "\n",
    "#X preprocessing\n",
    "imputer = SimpleImputer()\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "#Split X and Y into training and Test Sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_imputed, Y, shuffle=True)\n",
    "\n",
    "#Logistic Regression Model Setup\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "#Logistic Regression Model Metrics\n",
    "print(\"Logestic Regression\")\n",
    "print(\"Train Score: \", model.score(x_train, y_train))\n",
    "print(\"Test Score: \", model.score(x_test, y_test))\n",
    "print(classification_report(y_test, model.predict(x_test), digits=3))\n",
    "\n",
    "#Forest model setup\n",
    "forest = RandomForestClassifier(n_estimators=2, random_state=2)\n",
    "forest.fit(x_train, y_train)\n",
    "\n",
    "#Forest Model Metrics\n",
    "print(\"Forest Classifier\")\n",
    "print(\"Train Score: \", forest.score(x_train, y_train))\n",
    "print(\"Test Score: \", forest.score(x_test, y_test))\n",
    "print(classification_report(y_test, forest.predict(x_test), digits=3))\n",
    "\n",
    "print(y_test.shape)\n",
    "print(forest.predict(x_test).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "display(data_table.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "#make TF work like the TF in the HWs\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "#Other feature selections\n",
    "# X = pd.get_dummies(data_table[['HomeTeam', 'AwayTeam', 'HTeamEloScore', 'ATeamEloScore', 'HTdaysSinceLastMatch',\n",
    "#                             'ATdaysSinceLastMatch']])\n",
    "# X = pd.get_dummies(data_table[['HomeTeam', 'AwayTeam', 'FTAG', 'FTHG']])\n",
    "# X = pd.get_dummies(data_table[['HomeTeam', 'AwayTeam', 'FTR']])\n",
    "# X = pd.get_dummies(data_table.drop(['FTR',  'HTAG', 'Date', 'matchID', 'HW', 'AW', 'D', 'AR',\n",
    "#                                  'ordinalHR', 'Season'], axis=1))\n",
    "\n",
    "#Setup Data\n",
    "X = pd.get_dummies(data_table[['HomeTeam', 'AwayTeam', 'HTeamEloScore', 'ATeamEloScore', 'HTdaysSinceLastMatch',\n",
    "                            'ATdaysSinceLastMatch', 'HTW_rate', 'ATW_rate',\n",
    "                            'ATD_rate', 'HTD_rate', \n",
    "                '7_HTW_rate', '12_HTW_rate', '7_ATW_rate', '12_ATW_rate', \n",
    "                '7_HTD_rate', '12_HTD_rate', '7_ATD_rate', '12_ATD_rate',\n",
    "                '7_HTL_rate', '12_HTL_rate', '7_ATL_rate', '12_ATL_rate',\n",
    "                '5_HTHW_rate', '5_ATAW_rate']])\n",
    "\n",
    "# trasforma i valori delle vittorie come 2 e dei pareggi come 1\n",
    "Y = data_table[['ordinalHR']].to_numpy().ravel()*2\n",
    "\n",
    "\n",
    "#X preprocessing\n",
    "imputer = SimpleImputer()\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "#Split X and Y into training and Test Sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_imputed, Y, shuffle=True)\n",
    "\n",
    "#Neural Network Setup\n",
    "#Numero di feature del dataset\n",
    "n_inputs = X.shape[1]\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 3\n",
    "\n",
    "#Tensorflow X and Y\n",
    "X = tf.placeholder(tf.float32, shape = (None, n_inputs), name = 'X')\n",
    "Y = tf.placeholder(tf.int32, shape = (None), name = 'Y')\n",
    "\n",
    "#Function used to better display model metrics\n",
    "def convert_ordinalHR(x):\n",
    "    y=[]\n",
    "    for i in range(x.size):\n",
    "        if x[i] == 0:\n",
    "            y.append('A')\n",
    "        elif x[i] == 2:\n",
    "            y.append('H')\n",
    "        elif x[i] == 1:\n",
    "            y.append('D')\n",
    "    return y\n",
    "\n",
    "#General Function for neural layer setup\n",
    "def neuron_layer(X, n_neurons, name, activation = None):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        stddev = 2 / np.sqrt(n_inputs + n_neurons)\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)\n",
    "        W = tf.Variable(init, name = 'kernel')\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name='bias')\n",
    "        L2 = tf.nn.l2_loss(W)\n",
    "        Z = tf.matmul(X,W)+b\n",
    "        if activation is not None:\n",
    "            return activation(Z), L2\n",
    "        else:\n",
    "            return Z, L2\n",
    "\n",
    "#Tensorflow neural layer setup\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1, L2_1 = neuron_layer(X, n_hidden1, name='hidden1', activation=tf.nn.relu)\n",
    "    hidden2, L2_2 = neuron_layer(hidden1, n_hidden2, name='hidden2', activation=tf.nn.relu)\n",
    "    logits, L2_3 = neuron_layer(hidden2, n_outputs, name='outputs', activation=None)\n",
    "\n",
    "\n",
    "#Loss function\n",
    "beta=0.01\n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = Y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy+beta*(L2_1+L2_2+L2_3), name='loss')\n",
    "\n",
    "\n",
    "#Optimizer\n",
    "learning_rate = 0.000001\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "#Metric analysis setup\n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits, Y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    preds = tf.argmax(input=logits, axis=1)\n",
    "    \n",
    "#Initialize the above tensorflow variables    \n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#Runtime and batchsetup\n",
    "n_epochs = 30000\n",
    "batch_size = 50\n",
    "\n",
    "#Run the model\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    #Model Loop\n",
    "    for epoch in range(n_epochs):\n",
    "        sess.run(training_op, feed_dict={X: x_train, Y: y_train})\n",
    "        acc_train = accuracy.eval(feed_dict={X: x_train, Y: y_train})\n",
    "        acc_val = accuracy.eval(feed_dict={X: x_test, Y: y_test})\n",
    "        if(epoch % 200 == 0 or epoch==n_epochs-1):\n",
    "            print(epoch, 'Train accuracy:', acc_train, 'Val accuracy:', acc_val)\n",
    "    \n",
    "    #Retrive Metrics\n",
    "    acc_val = accuracy.eval(feed_dict={X: x_test, Y: y_test})\n",
    "    preds = preds.eval(feed_dict = {X:x_test})\n",
    "    log = logits.eval(feed_dict = {X:x_test})\n",
    "    preds=convert_ordinalHR(preds)\n",
    "    y_test=convert_ordinalHR(y_test)\n",
    "    \n",
    "    #Print Metrics\n",
    "    print('Train accuracy:', acc_train, 'Val accuracy:', acc_val)\n",
    "    print(classification_report(y_test, preds, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cgi import print_arguments\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "#Functions to manipulate data for use in the model\n",
    "# Home win = [1,0,0]\n",
    "# Away win = [0,1,0]\n",
    "# None win = [0,0,1]\n",
    "def one_hot_y(Y):\n",
    "    Y_new = np.zeros((Y.shape[0],3))\n",
    "    for i in range(Y.shape[0]-1):\n",
    "        if (Y[i] == 'H'):\n",
    "            Y_new[i]=[1,0,0]\n",
    "        elif (Y[i] == 'A'):\n",
    "            Y_new[i]=[0,1,0]\n",
    "        elif (Y[i] == 'D'):\n",
    "            Y_new[i]=[0,0,1]\n",
    "    return Y_new\n",
    "\n",
    "def revert_yoh(Y):\n",
    "    Y_new = np.empty([Y.shape[0],Y.shape[1]], dtype=\"<U1\")\n",
    "    #Y_new = np.zeros((Y.shape[0],Y.shape[1]))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            if (Y[i, j] == 0):\n",
    "                Y_new[i, j]= 'H'\n",
    "            elif (Y[i, j] == 1):\n",
    "                Y_new[i, j]= 'A'\n",
    "            elif (Y[i, j] == 2):\n",
    "                Y_new[i, j]='D'\n",
    "    return Y_new\n",
    "\n",
    "trn_ssn = [2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015]\n",
    "trn_ssn_len = len(trn_ssn)\n",
    "tst_ssn = [2016,2017,2018] \n",
    "tst_ssn_len = len(tst_ssn)\n",
    "    \n",
    "# è stata riscritta una funzione di reshape inutile \n",
    "# e.g. (a_prev=x_train, season=trn_ssn)\n",
    "def reshape_to_inputshape(a_prev,season):\n",
    "    totalMatches = len(season)*38\n",
    "    input_step = int(a_prev.shape[0]/totalMatches)\n",
    "    prev_f = a_prev.shape[1]\n",
    "    return np.reshape(a_prev, (totalMatches, input_step, prev_f))\n",
    "'''X = pd.get_dummies(data_table[['HomeTeam', 'AwayTeam', 'HTeamEloScore', 'ATeamEloScore', 'HTdaysSinceLastMatch',\n",
    "                            'ATdaysSinceLastMatch', 'HTdaysSinceLastMatch', 'ATdaysSinceLastMatch', 'HTW_rate', 'ATW_rate',\n",
    "                            'ATD_rate', 'HTD_rate', '1_last_HTR_isW', '1_last_HTR_isL', '1_last_ATR_isW', '1_last_ATR_isL']])'''\n",
    "\n",
    "#   Survived  Pclass     Sex   Age     Fare\n",
    "#0         0       3    male  22.0   7.2500\n",
    "#1         1       1  female  38.0  71.2833\n",
    "#2         1       3  female  26.0   7.9250\n",
    "#3         1       1  female  35.0  53.1000\n",
    "#4         0       3    male  35.0   8.0500\n",
    "# -----------------------------------------\n",
    "# get_dummies(dataset, column = ['sex']): associa codifica a valori testuali e scompone la colonna in pià colonne con le codifiche relative\n",
    "#   Survived  Pclass  Age     Fare  Sex_female  Sex_male\n",
    "#0         0       3   22   7.2500           0         1\n",
    "#1         1       1   38  71.2833           1         0\n",
    "#2         1       3   26   7.9250           1         0\n",
    "#3         1       1   35  53.1000           1         0\n",
    "#4         0       3   35   8.0500           0         1\n",
    "# -----------------------------------------\n",
    "# get_dummies(dataset['sex']):\n",
    "#   female  male\n",
    "#0       0     1\n",
    "#1       1     0\n",
    "#2       1     0\n",
    "#3       1     0\n",
    "#4       0     1\n",
    "features = ['HomeTeam', 'AwayTeam', \n",
    "            'HTeamEloScore', 'ATeamEloScore', \n",
    "            'HTdaysSinceLastMatch', 'ATdaysSinceLastMatch', \n",
    "            'HTW_rate', 'ATW_rate', 'ATD_rate', 'HTD_rate', \n",
    "            '7_HTW_rate', '12_HTW_rate', '7_ATW_rate', '12_ATW_rate', \n",
    "            '7_HTD_rate', '12_HTD_rate', '7_ATD_rate', '12_ATD_rate',\n",
    "            '7_HTL_rate', '12_HTL_rate', '7_ATL_rate', '12_ATL_rate',\n",
    "            '5_HTHW_rate', '5_ATAW_rate']\n",
    "X = pd.get_dummies(data_table[features])\n",
    "\n",
    "Y = data_table[['FTR']].to_numpy().ravel()\n",
    "\n",
    "#XY preprocessing\n",
    "# Replace missing values using a descriptive statistic (e.g. mean, median, or most frequent) along each column, or using a constant value\n",
    "# Default = mean\n",
    "imputer = SimpleImputer()\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "Y = one_hot_y(Y)\n",
    "\n",
    "test_size = float(tst_ssn_len)/(tst_ssn_len+trn_ssn_len)\n",
    "#Split X and Y into training and Test Sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_imputed, Y, shuffle=False, test_size=test_size)\n",
    "\n",
    "#Setup XY to have 10 game steps\n",
    "x_train = reshape_to_inputshape(x_train,trn_ssn)\n",
    "\n",
    "y_train = reshape_to_inputshape(y_train,trn_ssn)\n",
    "y_train = np.moveaxis(y_train, 0, 1)\n",
    "x_test = reshape_to_inputshape(x_test,tst_ssn)\n",
    "y_test = reshape_to_inputshape(y_test,tst_ssn)\n",
    "y_test = np.moveaxis(y_test, 0, 1)\n",
    "\n",
    "Tx= x_train.shape[1] #Time steps\n",
    "Ty= y_train.shape[0] #Time Steps\n",
    "num_features = x_train.shape[2] #Features per step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create and Setup Model\n",
    "fbmodel = tf.keras.Sequential()\n",
    "inputs = tf.keras.Input(shape=(Tx, num_features))\n",
    "outputs = []\n",
    "for t in range(Ty):\n",
    "    x = tf.keras.layers.Lambda(lambda z: inputs[:, t,:])(inputs)\n",
    "\n",
    "    x = tf.keras.layers.Reshape((1, num_features))(x)\n",
    "    \n",
    "    x = tf.keras.layers.LSTM(units=16, kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-1))(x)\n",
    "\n",
    "    x = tf.keras.layers.Dropout(rate=0.8)(x)\n",
    "    \n",
    "    out = tf.keras.layers.Dense(3, activation='softmax')(x)\n",
    "    \n",
    "    outputs.append(out)\n",
    "    \n",
    "fbmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "fbmodel.summary()\n",
    "\n",
    "fbmodel.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001)\n",
    "#    ,metrics=[tf.keras.metrics.Accuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Model\n",
    "history = fbmodel.fit(\n",
    "    x_train, list(y_train),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    #validation_split=0.272727,\n",
    "    #show = epoch%100==0,\n",
    "    verbose=1,\n",
    "    shuffle=False,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Metrics Data Setup\n",
    "y_pred = fbmodel.predict(x_test)\n",
    "y_predm = np.asarray(y_pred)\n",
    "print(\"shape y_predm\", y_predm.shape)\n",
    "y_predm = np.argmax(y_predm, axis=2)\n",
    "print(\"shape y_predm\", y_predm.shape)\n",
    "y_testm = np.argmax(y_test, axis = 2)\n",
    "print(\"shape y_testm\", y_testm.shape)\n",
    "\n",
    "# y_pred_train = fbmodel.predict(x_train)\n",
    "# y_pred_train = np.asarray(y_pred_train)\n",
    "# y_predm_train = np.argmax(y_pred_train, axis=2)\n",
    "# y_trainm = np.argmax(y_train, axis = 2)\n",
    "\n",
    "y_predm = revert_yoh(y_predm).ravel()\n",
    "print(\"shape y_predm\", y_predm.shape)\n",
    "y_testm = revert_yoh(y_testm).ravel()\n",
    "print(\"shape y_testm\", y_testm.shape)\n",
    "\n",
    "# y_predm_train = revert_yoh(y_predm_train).ravel()\n",
    "# y_trainm = revert_yoh(y_trainm).ravel()\n",
    "\n",
    "#Model Metrics\n",
    "# print('Train Score: ', accuracy_score(y_trainm,y_predm_train))\n",
    "# print('Test Score: ', accuracy_score(y_testm, y_predm))\n",
    "print(classification_report(y_testm, y_predm, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification report\n",
    "\n",
    "| precision | recall | f1-score | support |\n",
    "| ----------- | ------ | -------- | ------- |\n",
    "| A | 0.586 | 0.522 | 0.552 | 345 |\n",
    "| D | 0.252 | 0.123 | 0.165 | 253 |\n",
    "| H | 0.582 | 0.762 | 0.660 | 542 |\n",
    "| accuracy | |  | 0.547 | 1140 |\n",
    "| macro avg | 0.473 | 0.469 | 0.459 | 1140 |\n",
    "| weighted avg | 0.510 | 0.547 | 0.517 | 1140 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e35c5856cff72d20e6e0e19445c0d45b03888a7b19367444be448131dd51e693"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
